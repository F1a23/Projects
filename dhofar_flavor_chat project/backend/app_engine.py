import os
import re
import pickle
import difflib
from typing import List, Dict, Optional, Tuple
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, ChatOllama

load_dotenv()

# ===================== ENV =====================
DEBUG_MATCH = os.getenv("DEBUG_MATCH", "0") == "1"
CHAT_OLLAMA_MODEL = os.getenv("CHAT_OLLAMA_MODEL", "llama3.1")
EMBED_OLLAMA_MODEL = os.getenv("EMBED_OLLAMA_MODEL", "nomic-embed-text")
TOP_K = int(os.getenv("RAG_TOP_K", "5"))

BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # backend/
FAISS_PATH_ENV = os.getenv("FAISS_PATH", os.path.join("vectorstore", "faiss_index"))
FAISS_PATH = FAISS_PATH_ENV if os.path.isabs(FAISS_PATH_ENV) else os.path.join(BASE_DIR, FAISS_PATH_ENV)

# ===================== LOAD DATA =====================
PROJECT_ROOT = os.path.dirname(BASE_DIR)
RECIPES_PKL = os.path.join(PROJECT_ROOT, "data", "processed", "recipes.pkl")

if not os.path.exists(RECIPES_PKL):
    raise FileNotFoundError(f"recipes.pkl not found at:\n{RECIPES_PKL}")

with open(RECIPES_PKL, "rb") as f:
    RECIPES: List[Dict] = pickle.load(f)

STATE = {"options": None, "last_intent": "all"}  # list[Dict]

# ===================== NORMALIZE =====================
_AR_DIACRITICS = r"[\u064B-\u065F\u0670]"

def normalize_ar(text: str) -> str:
    if not text:
        return ""
    t = str(text)
    t = re.sub(_AR_DIACRITICS, "", t)
    t = t.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    t = t.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t

def canonical_term(term: str) -> str:
    t = normalize_ar(term)
    t = re.sub(r"[^0-9\u0621-\u064A ]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    if not t:
        return ""
    return t[2:] if t.startswith("Ø§Ù„") and len(t) > 2 else t

def is_short_term(msg: str) -> bool:
    return len((msg or "").split()) <= 2

# ===================== INTENT =====================
def detect_intent(q: str) -> str:
    t = normalize_ar(q)
    wants_ing = any(w in t for w in ["Ù…ÙƒÙˆÙ†Ø§Øª", "Ù…Ù‚Ø§Ø¯ÙŠØ±", "Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª", "ingredients"])
    wants_prep = any(w in t for w in ["Ø·Ø±ÙŠÙ‚Ø©", "ØªØ­Ø¶ÙŠØ±", "ÙƒÙŠÙ", "Ø§Ø³ÙˆÙŠ", "Ø§Ø·Ø¨Ø®", "Ø®Ø·ÙˆØ§Øª", "prep", "cook"])
    if wants_ing and wants_prep:
        return "all"
    if wants_ing:
        return "ingredients"
    if wants_prep:
        return "prep"
    return "all"

# ===================== KEYWORDS EXTRACT =====================
def extract_keywords_text(text: str) -> str:
    if not text:
        return ""
    parts = re.split(r"(?:Ø§Ù„ÙƒÙ„Ù…Ø§Øª\s+Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©|ÙƒÙ„Ù…Ø§Øª\s+Ù…ÙØªØ§Ø­ÙŠØ©)\s*:\s*", str(text), maxsplit=1)
    return parts[1].strip() if len(parts) == 2 else ""

def strip_keywords_anywhere(text: str) -> str:
    if not text:
        return ""
    return re.split(r"(?:Ø§Ù„ÙƒÙ„Ù…Ø§Øª\s+Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©|ÙƒÙ„Ù…Ø§Øª\s+Ù…ÙØªØ§Ø­ÙŠØ©)\s*:\s*", str(text), maxsplit=1)[0].strip()

def keywords_only_text(r: Dict) -> str:
    desc = r.get("description", "") or ""
    prep = r.get("prep", "") or ""
    kw = " ".join([extract_keywords_text(desc), extract_keywords_text(prep)]).strip()
    return normalize_ar(kw)

# ===================== NAME MATCH (STRONG) =====================
NAME_ALIASES = {
    "Ù‚Ù…Ø§Ø­Ù‡": ["Ø§Ù„Ù‚Ù…Ø§Ø­Ø©", "Ø§Ù„Ù‚Ù…Ø§Ø­Ù‡", "Ù‚Ù…Ø§Ø­Ø©", "Ù‚Ù…Ø§Ø­Ù‡", "Ù‚Ù…Ø­Ù‡", "Ø§Ù„Ù‚Ù…Ø­Ù‡"],
    "Ù‚Ø¨ÙˆÙ„ÙŠ": ["Ù‚Ø¨ÙˆÙ„ÙŠ", "Ø§Ù„Ù‚Ø¨ÙˆÙ„ÙŠ", "Ù‚Ø¨ÙˆÙ„Ø©", "Ø§Ù„Ù‚Ø¨ÙˆÙ„Ø©", "Ù‚Ø§Ø¨ÙˆÙ„ÙŠ"],
    "Ù…Ù‚Ø¯ÙŠØ¯": ["Ù…Ù‚Ø¯ÙŠØ¯", "Ø§Ù„Ù…Ù‚Ø¯ÙŠØ¯", "Ù…Ù‚Ø§Ø¯ÙŠØ¯", "Ø§Ù„Ù…Ù‚Ø§Ø¯ÙŠØ¯"],
}

def normalize_name(text: str) -> str:
    t = normalize_ar(text)

    # Ø§Ù„Ù‚Ù…Ø§Ø­Ø©
    t = t.replace("Ø§Ù„Ù‚Ù…Ø§Ø­Ø©", "Ø§Ù„Ù‚Ù…Ø§Ø­Ù‡").replace("Ù‚Ù…Ø§Ø­Ø©", "Ø§Ù„Ù‚Ù…Ø§Ø­Ù‡").replace("Ù‚Ù…Ø­Ù‡", "Ù‚Ù…Ø§Ø­Ù‡")

    # Ø§Ù„Ù‚Ø¨ÙˆÙ„ÙŠ
    t = t.replace("Ø§Ù„Ù‚Ø¨ÙˆÙ„ÙŠ", "Ù‚Ø¨ÙˆÙ„ÙŠ").replace("Ø§Ù„Ù‚Ø¨ÙˆÙ„Ø©", "Ù‚Ø¨ÙˆÙ„Ø©")

    # Ø§Ù„Ù…Ù‚Ø§Ø¯ÙŠØ¯/Ø§Ù„Ù…Ù‚Ø§Ø¯ÙŠØ¯ -> Ù…Ù‚Ø¯ÙŠØ¯
    t = t.replace("Ø§Ù„Ù…Ù‚Ø§Ø¯ÙŠØ¯", "Ù…Ù‚Ø¯ÙŠØ¯").replace("Ù…Ù‚Ø§Ø¯ÙŠØ¯", "Ù…Ù‚Ø¯ÙŠØ¯").replace("Ø§Ù„Ù…Ù‚Ø¯ÙŠØ¯", "Ù…Ù‚Ø¯ÙŠØ¯")

    parts = []
    for w in t.split():
        parts.append(w[2:] if w.startswith("Ø§Ù„") and len(w) > 2 else w)
    return " ".join(parts).strip()

def all_name_queries(q: str) -> List[str]:
    qn = normalize_name(q)
    out = {qn}
    for k, arr in NAME_ALIASES.items():
        kn = normalize_name(k)
        if qn == kn or qn in [normalize_name(x) for x in arr]:
            out.update([normalize_name(x) for x in arr])
    return [x for x in out if x]

def exact_name_hit(term: str, recipes: List[Dict]) -> Optional[Dict]:
    qs = all_name_queries(term)
    if not qs:
        return None
    for r in recipes:
        nm = normalize_name(r.get("name",""))
        if nm and nm in qs:
            return r
    return None

def find_by_name(term: str, recipes: List[Dict]) -> List[Dict]:
    qs = all_name_queries(term)
    if not qs:
        return []
    hits = []
    for r in recipes:
        nm = normalize_name(r.get("name",""))
        if not nm:
            continue
        if any(q == nm or q in nm or nm in q for q in qs):
            hits.append(r)
    hits.sort(key=lambda x: len(normalize_name(x.get("name",""))), reverse=True)
    return hits

def fuzzy_name_candidates(query: str, recipes: List[Dict], n: int = 6) -> List[Dict]:
    qn = normalize_name(query)
    names = [(normalize_name(r.get("name","")), r) for r in recipes if r.get("name")]
    name_only = [a for a, _ in names]
    close = difflib.get_close_matches(qn, name_only, n=n, cutoff=0.72)
    out = []
    for c in close:
        for a, r in names:
            if a == c:
                out.append(r)
    seen = set()
    uniq = []
    for r in out:
        k = normalize_name(r.get("name",""))
        if k in seen:
            continue
        seen.add(k)
        uniq.append(r)
    return uniq

# ==========================================================
# âœ… QABULI GUARD (Ù‚Ø¨ÙˆÙ„ÙŠ = Ø§Ø³Ù… Ø£ÙƒÙ„Ø© ÙÙ‚Ø·)
# ==========================================================
def is_qabuli_query(q: str) -> bool:
    qn = normalize_name(q)
    aliases = {normalize_name(x) for x in NAME_ALIASES.get("Ù‚Ø¨ÙˆÙ„ÙŠ", [])}
    return qn in aliases

def find_qabuli_by_name_only(recipes: List[Dict]) -> List[Dict]:
    keys = ["Ù‚Ø¨ÙˆÙ„ÙŠ", "Ù‚Ø¨ÙˆÙ„Ø©", "Ù‚Ø§Ø¨ÙˆÙ„ÙŠ"]
    hits = []
    for r in recipes:
        nm = normalize_name(r.get("name",""))
        if not nm:
            continue
        if any(k in nm for k in keys):
            hits.append(r)

    seen = set()
    uniq = []
    for r in hits:
        nm = normalize_name(r.get("name",""))
        if nm in seen:
            continue
        seen.add(nm)
        uniq.append(r)

    uniq.sort(key=lambda x: (0 if normalize_name(x.get("name","")) == "Ù‚Ø¨ÙˆÙ„ÙŠ" else 1, len(normalize_name(x.get("name","")))))
    return uniq

# ===================== STRICT INGREDIENT MATCH =====================
def ingredient_patterns(term: str) -> List[str]:
    t = canonical_term(term)
    if not t:
        return []
    if t in {"Ù„Ø­Ù…", "Ù„Ø­ÙˆÙ…"}:
        return ["Ù„Ø­Ù…", "Ù„Ø­Ù…Ù‡", "Ù„Ø­ÙˆÙ…", "Ø§Ù„Ù„Ø­Ù…", "Ø§Ù„Ù„Ø­Ù…Ù‡", "Ø§Ù„Ù„Ø­ÙˆÙ…"]
    if t in {"Ø³Ù…Ùƒ", "Ø§Ø³Ù…Ø§Ùƒ", "Ø£Ø³Ù…Ø§Ùƒ"}:
        return ["Ø³Ù…Ùƒ", "Ø§Ù„Ø³Ù…Ùƒ", "Ø§Ø³Ù…Ø§Ùƒ", "Ø£Ø³Ù…Ø§Ùƒ", "Ø³Ø±Ø¯ÙŠÙ†", "ØªÙˆÙ†Ù‡", "ØªÙˆÙ†Ø©", "Ø±ÙˆØ¨ÙŠØ§Ù†", "Ø¬Ù…Ø¨Ø±ÙŠ", "Ø±Ø¨ÙŠØ§Ù†", "Ø­Ø¨Ø§Ø±", "Ù‚Ø±Ø´"]
    return [t, "Ø§Ù„" + t]

def word_boundary_regex(words: List[str]) -> re.Pattern:
    cleaned = [canonical_term(w) for w in words if canonical_term(w)]
    if not cleaned:
        return re.compile(r"a^")
    alt = "|".join(re.escape(w) for w in cleaned)
    pat = rf"(^|[^\u0621-\u064A0-9])({alt})([^\u0621-\u064A0-9]|$)"
    return re.compile(pat)

def ingredient_hit_reason(term: str, r: Dict) -> Optional[str]:
    pats = ingredient_patterns(term)
    rx = word_boundary_regex(pats)

    ing_list = r.get("ingredients") or []
    if not isinstance(ing_list, list):
        ing_list = [str(ing_list)]

    q = canonical_term(term)

    false_positive_context = {
        "Ø¨Ù‡Ø§Ø±Ø§Øª", "ØªÙˆØ§Ø¨Ù„", "Ø¨Ø²Ø§Ø±", "Ù…Ø±Ù‚", "Ù…ÙƒØ¹Ø¨", "Ù…ÙƒØ¹Ø¨Ø§Øª", "Ø¨ÙˆØ¯Ø±Ø©", "Ù…Ø³Ø­ÙˆÙ‚",
        "Ø®Ù„Ø·Ù‡", "Ø®Ù„Ø·Ø©", "ØªØªØ¨ÙŠÙ„Ø©", "ØªØªØ¨ÙŠÙ„Ù‡"
    }

    for ing in ing_list:
        original = str(ing)
        line = normalize_ar(original)

        if not rx.search(line):
            continue

        if q in {"Ù„Ø­Ù…", "Ù„Ø­ÙˆÙ…"}:
            tokens = set(re.sub(r"[^0-9\u0621-\u064A ]+", " ", line).split())
            if any(fp in tokens for fp in false_positive_context):
                continue

        return original

    return None

def find_by_ingredient_strict(term: str, recipes: List[Dict]) -> List[Tuple[Dict, str]]:
    out = []
    for r in recipes:
        reason = ingredient_hit_reason(term, r)
        if reason:
            out.append((r, reason))

    seen = set()
    uniq = []
    for r, reason in out:
        k = normalize_name(r.get("name",""))
        if k in seen:
            continue
        seen.add(k)
        uniq.append((r, reason))

    uniq.sort(key=lambda x: normalize_name(x[0].get("name","")))
    return uniq

# ===================== KEYWORDS STRICT =====================
def keyword_match_strict(query: str, kw_text: str) -> bool:
    if not kw_text:
        return False
    qn = normalize_ar(query)
    qn = re.sub(r"[^0-9\u0621-\u064A ]+", " ", qn)
    qn = re.sub(r"\s+", " ", qn).strip()
    if not qn:
        return False

    words = [canonical_term(w) for w in qn.split() if canonical_term(w)]
    if not words:
        return False

    for w in words:
        pat = re.compile(rf"(^|\s){re.escape(w)}(\s|$)")
        if not pat.search(kw_text):
            return False
    return True

def find_by_keywords_strict(term: str, recipes: List[Dict]) -> List[Tuple[Dict, str]]:
    out = []
    for r in recipes:
        kw = keywords_only_text(r)
        if kw and keyword_match_strict(term, kw):
            out.append((r, kw))

    seen = set()
    uniq = []
    for r, kw in out:
        k = normalize_name(r.get("name",""))
        if k in seen:
            continue
        seen.add(k)
        uniq.append((r, kw))

    uniq.sort(key=lambda x: normalize_name(x[0].get("name","")))
    return uniq

# ===================== SPECIAL: BAKED GOODS =====================
BAKED_TRIGGER = {"Ù…Ø®Ø¨ÙˆØ²Ø§Øª", "Ù…Ø®Ø¨ÙˆØ²", "Ø®Ø¨Ø²", "Ø¹ÙŠØ´", "Ø±Ù‚Ø§Ù‚", "Ù„Ø­ÙˆØ­", "Ø±ØºÙŠÙ"}

def is_bread_recipe(r: Dict) -> bool:
    nm_raw = r.get("name","") or ""
    nm = normalize_ar(nm_raw)
    tp = normalize_ar(r.get("type","") or "")
    kw = keywords_only_text(r)

    if "Ø¹ÙŠØ´ Ø¨Ø§Ù„Ù†Ø§Ø±Ø¬ÙŠÙ„" in nm:
        return False

    if any(w in nm for w in ["Ø®Ø¨Ø²", "Ø±Ù‚Ø§Ù‚", "Ù„Ø­ÙˆØ­", "Ø±ØºÙŠÙ"]):
        return True

    if "Ø¹ÙŠØ´" in nm:
        if "Ø®Ø¨Ø²" in tp or "Ù…Ø®Ø¨ÙˆØ²" in tp:
            return True
        if keyword_match_strict("Ø®Ø¨Ø²", kw) or keyword_match_strict("Ù…Ø®Ø¨ÙˆØ²Ø§Øª", kw):
            return True
        return False

    if "Ø®Ø¨Ø²" in tp or "Ù…Ø®Ø¨ÙˆØ²" in tp:
        return True

    if keyword_match_strict("Ø®Ø¨Ø²", kw) or keyword_match_strict("Ù…Ø®Ø¨ÙˆØ²Ø§Øª", kw):
        return True

    return False

def find_baked_goods(recipes: List[Dict]) -> List[Dict]:
    out = [r for r in recipes if is_bread_recipe(r)]
    out.sort(key=lambda x: normalize_name(x.get("name","")))
    return out

# ===================== SPECIAL: SWEETS =====================
SWEET_TRIGGER = {"Ø­Ù„ÙˆÙŠØ§Øª", "Ø­Ù„Ø§", "ØªØ­Ù„ÙŠÙ‡", "ØªØ­Ù„ÙŠØ©", "Ø­Ù„ÙˆÙ‰", "Ø­Ù„Ùˆ", "ÙƒÙŠÙƒ", "ÙƒØ¹Ùƒ", "ÙƒØ¹ÙƒÙ‡", "ÙƒØ¹ÙƒØ©", "Ø¨Ø³Ø¨ÙˆØ³Ù‡", "Ù„Ù‚ÙŠÙ…Ø§Øª"}
SWEET_NAME_FORCE = {"Ø§Ù„Ù‚Ø´Ø§Ø·", "Ù‚Ø´Ø§Ø·", "Ø§Ù„Ù„Ø¨Ù†ÙŠÙ‡", "Ù„Ø¨Ù†ÙŠÙ‡", "Ø§Ù„Ù„Ø¨Ù†ÙŠØ©", "Ù„Ø¨Ù†ÙŠØ©", "Ø§Ù„Ø¨Ù†ÙŠÙ‡", "Ø¨Ù†ÙŠÙ‡", "Ø§Ù„Ø¨Ù†ÙŠØ©", "Ø¨Ù†ÙŠØ©"}

def is_sweet_recipe(r: Dict) -> bool:
    nm_raw = r.get("name","") or ""
    nm = normalize_ar(nm_raw)
    tp = normalize_ar(r.get("type","") or "")
    kw = keywords_only_text(r)

    if normalize_name(nm_raw) in {normalize_name(x) for x in SWEET_NAME_FORCE}:
        return True

    if "ØªØ­Ù„ÙŠÙ‡" in tp or "ØªØ­Ù„ÙŠØ©" in tp:
        return True

    if any(w in nm for w in ["ÙƒÙŠÙƒ", "ÙƒØ¹Ùƒ", "ÙƒØ¹ÙƒÙ‡", "ÙƒØ¹ÙƒØ©", "Ø¨Ø³Ø¨ÙˆØ³Ù‡", "Ù„Ù‚ÙŠÙ…Ø§Øª", "Ø­Ù„ÙˆÙŠØ§Øª", "Ø­Ù„Ø§", "Ø­Ù„ÙˆÙ‰"]):
        return True

    for w in ["Ø­Ù„ÙˆÙŠØ§Øª", "Ø­Ù„Ø§", "ØªØ­Ù„ÙŠÙ‡", "ØªØ­Ù„ÙŠØ©", "Ø­Ù„ÙˆÙ‰", "ÙƒÙŠÙƒ", "ÙƒØ¹Ùƒ", "Ø¨Ø³Ø¨ÙˆØ³Ù‡", "Ù„Ù‚ÙŠÙ…Ø§Øª", "Ù‚Ø´Ø§Ø·", "Ù„Ø¨Ù†ÙŠÙ‡", "Ù„Ø¨Ù†ÙŠØ©", "Ø¨Ù†ÙŠÙ‡", "Ø¨Ù†ÙŠØ©"]:
        if keyword_match_strict(w, kw):
            return True

    return False

def find_sweets(recipes: List[Dict]) -> List[Dict]:
    out = [r for r in recipes if is_sweet_recipe(r)]
    out.sort(key=lambda x: normalize_name(x.get("name","")))
    return out

# ===================== âœ… SPECIAL: DRINKS (STRICT - YOUR NAMES) =====================
# Trigger words
DRINK_TRIGGER_WORDS = {"Ù…Ø´Ø±ÙˆØ¨", "Ù…Ø´Ø±ÙˆØ¨Ø§Øª", "Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª", "Ø´Ø±Ø§Ø¨", "Ø§Ø´Ø±Ø¨Ø©", "Ø£Ø´Ø±Ø¨Ø©", "Ø§Ø´Ø±Ø¨Ù‡"}

# âœ… Ø£Ø³Ù…Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¨Ø§ØªÙƒ Ø§Ù„ÙØ¹Ù„ÙŠØ© (Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø§ØªØ§)
DRINK_EXACT_NAMES = [
    "Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨ (Ø§Ù„Ù„Ø¨Ù† Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨)",
    "Ø´Ø§Ù‡ÙŠ Ø­Ù„ÙŠØ¨ ",
]

def _norm(s: str) -> str:
    return normalize_name(s or "")

def find_drinks(recipes: List[Dict]) -> List[Dict]:
    """
    ØµØ§Ø±Ù… Ø¬Ø¯Ù‹Ø§:
    - ÙŠØ±Ø¬Ø¹ ÙÙ‚Ø·: Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨ + Ø´Ø§Ù‡ÙŠ Ø­Ù„ÙŠØ¨ Ø­Ù„Ùˆ
    - ÙˆÙŠØ³ØªØ¨Ø¹Ø¯ Ø£ÙŠ Ø´ÙŠØ¡ ÙÙŠÙ‡ Ù„Ø¨Ù†ÙŠØ© (Ù…Ø«Ù„: Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ÙƒØ²ÙŠØ¨)
    """
    target_set = {_norm(x) for x in DRINK_EXACT_NAMES if _norm(x)}
    hits = []

    for r in recipes:
        nm = _norm(r.get("name",""))
        if not nm:
            continue

        # âŒ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø£ÙŠ "Ù„Ø¨Ù†ÙŠØ©"
        if "Ù„Ø¨Ù†ÙŠÙ‡" in normalize_ar(r.get("name","") or "") or "Ù„Ø¨Ù†ÙŠØ©" in (r.get("name","") or ""):
            continue

        # âœ… Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
        if nm in target_set:
            hits.append(r)

    # ØªØ±ØªÙŠØ¨ Ø«Ø§Ø¨Øª: Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨ Ø£ÙˆÙ„Ø§Ù‹ Ø«Ù… Ø§Ù„Ø´Ø§Ù‡ÙŠ
    order = {_norm("Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨ (Ø§Ù„Ù„Ø¨Ù† Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨)"): 0, _norm("Ø´Ø§Ù‡ÙŠ Ø­Ù„ÙŠØ¨ Ø­Ù„Ùˆ"): 1}
    hits.sort(key=lambda r: order.get(_norm(r.get("name","")), 99))
    return hits

# ===================== DISPLAY =====================
def format_recipe(r: Dict, intent: str = "all") -> str:
    r = dict(r)
    r["description"] = strip_keywords_anywhere(r.get("description", ""))
    r["prep"] = strip_keywords_anywhere(r.get("prep", ""))

    out = f"ğŸ² {r.get('name','')}\n"
    if r.get("type"): out += f"Ù†ÙˆØ¹ Ø§Ù„Ø£ÙƒÙ„Ø©: {r['type']}\n"
    if r.get("region"): out += f"Ø§Ù„Ù…Ù†Ø·Ù‚Ø©: {r['region']}\n"
    if r.get("cook_method"): out += f"Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø·Ù‡ÙŠ: {r['cook_method']}\n"

    if intent in ("all", "ingredients"):
        if r.get("ingredients"):
            out += "\n\nğŸ§‚ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª:\n" + "\n".join([f"- {x}" for x in r["ingredients"]])
        elif intent == "ingredients":
            out += "\n\nğŸ§‚ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª:\nØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"

    if intent in ("all", "prep"):
        if r.get("prep"):
            out += "\n\nğŸ‘©â€ğŸ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±:\n" + r["prep"].strip()
        elif intent == "prep":
            out += "\n\nğŸ‘©â€ğŸ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±:\nØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"

    if intent == "all":
        if r.get("description"):
            out += "\n\nğŸ“ Ø§Ù„ÙˆØµÙ:\n" + r["description"].strip()

    return out.strip()

def list_names(prefix: str, items: List[Dict], max_items: int = 80) -> str:
    out = prefix.strip() + f"\n\nğŸ”¢ Ø§Ù„Ø¹Ø¯Ø¯: {len(items)}\n\n"
    for i, r in enumerate(items[:max_items], 1):
        out += f"{i}) {r.get('name','')}\n"
    if len(items) > max_items:
        out += f"\nâ€¦ (Ø¹Ø±Ø¶Ù†Ø§ Ø£ÙˆÙ„ {max_items} ÙÙ‚Ø·)\n"
    out += "\nâœï¸ Ø§ÙƒØªØ¨ÙŠ Ø±Ù‚Ù… Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±."
    return out

# ===================== LONG QUESTION -> EXTRACT DISH =====================
def extract_candidate_dish(q: str) -> str:
    t = normalize_ar(q)
    t = re.sub(r"\b(ÙƒÙŠÙ|Ø·Ø±ÙŠÙ‚Ø©|ØªØ­Ø¶ÙŠØ±|Ø§Ø³ÙˆÙŠ|Ø§Ø³ÙˆÙŠÙ‡Ø§|Ø§Ø·Ø¨Ø®|Ø§Ø¹Ù…Ù„|Ø¹Ù…Ù„|Ù…ÙƒÙˆÙ†Ø§Øª|Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª|Ù…Ù‚Ø§Ø¯ÙŠØ±|ÙˆØµÙÙ‡|Ø§Ù„ÙˆØµÙÙ‡|ÙˆØ´|Ø§ÙŠØ´|Ø§Ø±ÙŠØ¯|Ø§Ø¨ÙŠ)\b", " ", t)
    t = re.sub(r"[^0-9\u0621-\u064A ]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    if not t:
        return q
    parts = t.split()
    return " ".join(parts[-3:]).strip()

# ===================== RAG (FAISS + OLLAMA) =====================
def _load_faiss():
    if not os.path.exists(FAISS_PATH):
        return None
    embeddings = OllamaEmbeddings(model=EMBED_OLLAMA_MODEL)
    return FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)

_FAISS_DB = None

def rag_answer(question: str) -> str:
    global _FAISS_DB
    if _FAISS_DB is None:
        _FAISS_DB = _load_faiss()

    if _FAISS_DB is None:
        return "âš ï¸ ÙÙ‡Ø±Ø³ FAISS ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø´ØºÙ‘Ù„ÙŠ build_faiss_index.py Ø£ÙˆÙ„Ø§Ù‹."

    retriever = _FAISS_DB.as_retriever(search_kwargs={"k": TOP_K})
    docs = retriever.get_relevant_documents(question)
    context = "\n\n---\n\n".join([d.page_content for d in docs])[:12000]

    llm = ChatOllama(model=CHAT_OLLAMA_MODEL)

    prompt = (
        "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ø¨Ø® Ø¸ÙØ§Ø±ÙŠ. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©.\n"
        "Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù‚Ù„: ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ.\n\n"
        f"Ø§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\n"
        f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n"
        "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
    )

    resp = llm.invoke(prompt)
    return (resp.content or "").strip() if hasattr(resp, "content") else str(resp).strip()

# ===================== MAIN ROUTER =====================
def answer_question(question: str) -> str:
    q = (question or "").strip()
    if not q:
        return "Ø§ÙƒØªØ¨ÙŠ: Ù…ÙƒÙˆÙ‘Ù† (Ù„Ø­Ù…/Ø³Ù…Ùƒ..) Ø£Ùˆ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø£Ùˆ (Ù…Ø®Ø¨ÙˆØ²Ø§Øª) Ø£Ùˆ (Ø­Ù„ÙˆÙŠØ§Øª) Ø£Ùˆ (Ù…Ø´Ø±ÙˆØ¨) Ø£Ùˆ Ø§Ø³Ù… Ø£ÙƒÙ„Ø©."

    intent = detect_intent(q)
    qnorm = canonical_term(q)

    # (0) Ø§Ø®ØªÙŠØ§Ø± Ø±Ù‚Ù…
    if q.isdigit() and STATE.get("options"):
        idx = int(q) - 1
        opts = STATE["options"] or []
        if 0 <= idx < len(opts):
            picked = opts[idx]
            STATE["options"] = None
            chosen_intent = STATE.get("last_intent") or "all"
            return format_recipe(picked, chosen_intent)
        return "Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©."

    # âœ… (D) Ù…Ø´Ø±ÙˆØ¨Ø§Øª â€” ØµØ§Ø±Ù…: ÙŠØ±Ø¬Ø¹ ÙÙ‚Ø· (Ø§Ù„Ù…Ø¹Ø°ÙŠØ¨ + Ø´Ø§Ù‡ÙŠ Ø­Ù„ÙŠØ¨ Ø­Ù„Ùˆ) ÙˆÙŠØ³ØªØ¨Ø¹Ø¯ "Ù„Ø¨Ù†ÙŠØ©"
    qN = normalize_ar(q)
    if any(w in qN.split() for w in DRINK_TRIGGER_WORDS):
        drinks = find_drinks(RECIPES)
        if drinks:
            STATE["options"] = drinks
            STATE["last_intent"] = "all"
            return list_names("âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª ÙÙ‚Ø·ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", drinks, 80)
        return "ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"

    # âœ…âœ…âœ… Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù‚Ø¨ÙˆÙ„ÙŠ: Ø§Ø³Ù… Ø£ÙƒÙ„Ø© ÙÙ‚Ø·
    if is_short_term(q) and is_qabuli_query(q):
        ex = exact_name_hit(q, RECIPES) or exact_name_hit("Ù‚Ø¨ÙˆÙ„ÙŠ", RECIPES)
        if ex:
            STATE["options"] = None
            return format_recipe(ex, intent)

        qabuli_hits = find_qabuli_by_name_only(RECIPES)
        if qabuli_hits:
            if len(qabuli_hits) == 1:
                STATE["options"] = None
                return format_recipe(qabuli_hits[0], intent)
            STATE["options"] = qabuli_hits
            STATE["last_intent"] = intent
            return list_names("âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙƒØ«Ø± Ù…Ù† Ø£ÙƒÙ„Ø© Ø¨Ø§Ø³Ù… (Ù‚Ø¨ÙˆÙ„ÙŠ/Ù‚Ø¨ÙˆÙ„Ø©/Ù‚Ø§Ø¨ÙˆÙ„ÙŠ)ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", qabuli_hits, 80)

        fuzzy = fuzzy_name_candidates("Ù‚Ø¨ÙˆÙ„ÙŠ", RECIPES) or fuzzy_name_candidates(q, RECIPES)
        if fuzzy:
            if len(fuzzy) == 1:
                STATE["options"] = None
                return format_recipe(fuzzy[0], intent)
            STATE["options"] = fuzzy
            STATE["last_intent"] = intent
            return list_names("âœ… Ù„Ù‚ÙŠØª Ø£Ø³Ù…Ø§Ø¡ Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† (Ù‚Ø¨ÙˆÙ„ÙŠ)ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", fuzzy, 80)

        STATE["options"] = None
        return "ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"

    # (A) Ø³Ø¤Ø§Ù„ Ø·ÙˆÙŠÙ„ -> Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø£ÙƒÙ„Ø© Ø«Ù… RAG
    if not is_short_term(q):
        cand = extract_candidate_dish(q)

        ex = exact_name_hit(cand, RECIPES) or exact_name_hit(q, RECIPES)
        if ex:
            STATE["options"] = None
            return format_recipe(ex, intent)

        hits = find_by_name(cand, RECIPES) or find_by_name(q, RECIPES)
        if not hits:
            hits = fuzzy_name_candidates(cand, RECIPES) or fuzzy_name_candidates(q, RECIPES)

        if hits:
            if len(hits) == 1:
                STATE["options"] = None
                return format_recipe(hits[0], intent)
            STATE["options"] = hits
            STATE["last_intent"] = intent
            return list_names("âœ… Ù„Ù‚ÙŠØª Ø£ÙƒØ«Ø± Ù…Ù† Ø£ÙƒÙ„Ø© Ù…Ø­ØªÙ…Ù„Ø© Ù…Ù† Ø³Ø¤Ø§Ù„ÙƒØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", hits, 80)

        # Ù„Ùˆ Ù…Ø§ Ù„Ù‚Ù‰ Ø§Ø³Ù… Ø£ÙƒÙ„Ø© â†’ RAG
        return rag_answer(q)

    # (B) Ù…Ø®Ø¨ÙˆØ²Ø§Øª
    if qnorm in BAKED_TRIGGER:
        baked = find_baked_goods(RECIPES)
        if baked:
            STATE["options"] = baked
            STATE["last_intent"] = "all"
            return list_names("âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø®Ø¨ÙˆØ²Ø§Øª/Ø§Ù„Ø®Ø¨Ø² ÙÙ‚Ø·ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", baked, 80)
        return "ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"

    # (C) Ø­Ù„ÙˆÙŠØ§Øª
    if qnorm in SWEET_TRIGGER:
        sweets = find_sweets(RECIPES)
        if sweets:
            STATE["options"] = sweets
            STATE["last_intent"] = "all"
            return list_names("âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø­Ù„ÙˆÙŠØ§Øª/Ø§Ù„Ø£Ø·Ø¨Ø§Ù‚ Ø§Ù„Ø­Ù„ÙˆØ© ÙÙ‚Ø·ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", sweets, 80)
        return "ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"

    # ==========================================================
    # ØªØ±ØªÙŠØ¨Ùƒ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
    # 1) INGREDIENTS
    # 2) KEYWORDS
    # 3) NAMES
    # ==========================================================

    # (1) INGREDIENTS FIRST (STRICT)
    if is_short_term(q):
        ing_hits_with_reason = find_by_ingredient_strict(q, RECIPES)
        if ing_hits_with_reason:
            ing_hits = [r for r, _ in ing_hits_with_reason]

            if DEBUG_MATCH:
                dbg = f"âœ… (DEBUG) Ø£ÙƒÙ„Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ({q}) Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ÙÙ‚Ø·:\n\n"
                dbg += f"ğŸ”¢ Ø§Ù„Ø¹Ø¯Ø¯: {len(ing_hits)}\n\n"
                for i, r in enumerate(ing_hits[:80], 1):
                    dbg += f"{i}) {r.get('name','')}\n"
                dbg += "\nâœï¸ Ø§ÙƒØªØ¨ÙŠ Ø±Ù‚Ù… Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±."
                STATE["options"] = ing_hits
                STATE["last_intent"] = "all"
                return dbg.strip()

            STATE["options"] = ing_hits
            STATE["last_intent"] = "all"
            return list_names(f"âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙƒÙ„Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ({q}) Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª/Ø§Ù„Ù…Ù‚Ø§Ø¯ÙŠØ± ÙÙ‚Ø·ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", ing_hits, 80)

    # (2) KEYWORDS SECOND (STRICT)
    if is_short_term(q):
        kw_hits_with_reason = find_by_keywords_strict(q, RECIPES)
        if kw_hits_with_reason:
            kw_hits = [r for r, _ in kw_hits_with_reason]
            STATE["options"] = kw_hits
            STATE["last_intent"] = "all"
            return list_names(f"âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙƒÙ„Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ({q})ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", kw_hits, 80)

    # (3) NAMES LAST
    if is_short_term(q):
        ex = exact_name_hit(q, RECIPES)
        if ex:
            STATE["options"] = None
            return format_recipe(ex, intent)

        name_hits = find_by_name(q, RECIPES)
        if not name_hits:
            name_hits = fuzzy_name_candidates(q, RECIPES)

        if name_hits:
            if len(name_hits) == 1:
                STATE["options"] = None
                return format_recipe(name_hits[0], intent)
            STATE["options"] = name_hits
            STATE["last_intent"] = intent
            return list_names("âœ… ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙƒØ«Ø± Ù…Ù† Ø£ÙƒÙ„Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø§Ø³Ù…ØŒ Ø§Ø®ØªØ§Ø±ÙŠ Ø±Ù‚Ù…:", name_hits, 80)

    return "ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØµØ§Ø¯Ø±ÙŠ"
