{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a3defe-b74a-4090-bab6-5820a89b8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\n",
      "Raw data folder: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\data\\raw\n",
      "Domain documents folder: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\data\\domain_documents\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# This cell creates the recommended project folders and renames \"interface\" to \"client\" if it exists.\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rename interface -> client (only if interface exists and client does not)\n",
    "interface_dir = PROJECT_ROOT / \"interface\"\n",
    "client_dir = PROJECT_ROOT / \"client\"\n",
    "if interface_dir.exists() and not client_dir.exists():\n",
    "    interface_dir.rename(client_dir)\n",
    "\n",
    "folders_to_create = [\n",
    "    PROJECT_ROOT / \"backend\",                     # Backend code (Python + RAG + Ollama calls)\n",
    "    PROJECT_ROOT / \"client\",                      # UI layer (Gradio/Streamlit/Web)\n",
    "\n",
    "    PROJECT_ROOT / \"data\" / \"raw\",                # Raw sources (docx/pdf/xlsx)\n",
    "    PROJECT_ROOT / \"data\" / \"processed\",          # Cleaned text outputs (optional)\n",
    "\n",
    "    PROJECT_ROOT / \"data\" / \"domain_documents\" / \"traditional_dishes\",      # Main dishes (one dish per file)\n",
    "    PROJECT_ROOT / \"data\" / \"domain_documents\" / \"desserts\",                # Desserts (one dish per file)\n",
    "    PROJECT_ROOT / \"data\" / \"domain_documents\" / \"cooking_methods\",         # Cooking methods explanations\n",
    "    PROJECT_ROOT / \"data\" / \"domain_documents\" / \"ingredients_reference\",   # Ingredients/spices reference\n",
    "    PROJECT_ROOT / \"data\" / \"domain_documents\" / \"metadata\",                # Sources and notes\n",
    "\n",
    "    PROJECT_ROOT / \"vectorstore\",                 # Vector database files (Chroma/FAISS/Qdrant)\n",
    "    PROJECT_ROOT / \"rag_store\",                   # Cached chunks/exports (optional)\n",
    "    PROJECT_ROOT / \"exports\",                     # Output exports\n",
    "    PROJECT_ROOT / \"logs\",                        # Logs\n",
    "    PROJECT_ROOT / \"notebooks\",                   # Jupyter notebooks\n",
    "]\n",
    "\n",
    "for p in folders_to_create:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Raw data folder:\", PROJECT_ROOT / \"data\" / \"raw\")\n",
    "print(\"Domain documents folder:\", PROJECT_ROOT / \"data\" / \"domain_documents\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a43e95d7-fc87-49e2-a9bf-07c8a2aae535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.14.2\n",
      "\n",
      "NAME                       ID              SIZE      MODIFIED       \n",
      "llama3:latest              365c0bd3c000    4.7 GB    20 minutes ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    6 days ago        \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    6 days ago        \n",
      "llama2:latest              78e26419b446    3.8 GB    7 days ago        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check that Ollama is installed and running\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_cmd(cmd: str) -> None:\n",
    "    # Run a shell command and print its output\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Show Ollama version\n",
    "run_cmd(\"ollama -v\")\n",
    "\n",
    "# List installed local models\n",
    "run_cmd(\"ollama list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef515ff-96dc-4393-86dd-d0602b374daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STDERR: \u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 6a0746a1ec1a: 100% ▕██████████████████▏ 4.7 GB                         \u001b[K\n",
      "pulling 4fa551d4f938: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 8ab4849b038c: 100% ▕██████████████████▏  254 B                         \u001b[K\n",
      "pulling 577073ffcc6c: 100% ▕██████████████████▏  110 B                         \u001b[K\n",
      "pulling 3f8eb4da87fa: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Download the required Ollama model\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_cmd(cmd: str) -> None:\n",
    "    # Run a shell command and print its output\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        shell=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Download the llama3 model locally via Ollama\n",
    "run_cmd(\"ollama pull llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144fd8c1-bc1c-4634-8486-6388006123e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\ollama_client.py\n"
     ]
    }
   ],
   "source": [
    "# Save Ollama text generation helper into the backend folder\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Project root path\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Target file path\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"ollama_client.py\"\n",
    "\n",
    "# Ensure backend folder exists\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Code to be saved\n",
    "code = \"\"\"\\\n",
    "# Ollama text generation helper for the RAG pipeline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def ollama_generate(prompt: str, model: str = \"llama3\") -> str:\n",
    "    # Run the model and return the generated text\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    return (result.stdout or \"\").strip()\n",
    "\"\"\"\n",
    "\n",
    "# Write file\n",
    "TARGET_FILE.write_text(code, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"File saved successfully at: {TARGET_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9ac4d20-3d81-4b47-a0b2-9b4e6a71ae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\ollama_client.py\n",
      "File exists: True\n",
      "\n",
      "--- File content (first 200 lines) ---\n",
      "\n",
      "# Ollama text generation helper for the RAG pipeline\n",
      "\n",
      "import subprocess\n",
      "\n",
      "def ollama_generate(prompt: str, model: str = \"llama3\") -> str:\n",
      "    # Run the model and return the generated text\n",
      "    result = subprocess.run(\n",
      "        [\"ollama\", \"run\", model],\n",
      "        input=prompt,\n",
      "        text=True,\n",
      "        capture_output=True,\n",
      "        encoding=\"utf-8\",\n",
      "        errors=\"ignore\"\n",
      "    )\n",
      "    return (result.stdout or \"\").strip()\n"
     ]
    }
   ],
   "source": [
    "# Inspect the Ollama client file and preview its content\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the project root path\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Point to the Ollama client file\n",
    "p = PROJECT_ROOT / \"backend\" / \"ollama_client.py\"\n",
    "\n",
    "# Print the file path\n",
    "print(\"File path:\", p)\n",
    "\n",
    "# Check if the file exists\n",
    "print(\"File exists:\", p.exists())\n",
    "\n",
    "# Print the first 200 lines of the file for inspection\n",
    "print(\"\\n--- File content (first 200 lines) ---\\n\")\n",
    "print(\"\\n\".join(\n",
    "    p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()[:200]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef4fdfc5-fb9d-4934-bae4-51c7a1bb457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions found: ['ollama_generate', 'ollama_generate_one_word_ar']\n",
      "مرحبا\n"
     ]
    }
   ],
   "source": [
    "# Reload the backend module and run a quick test to avoid using an old cached version\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\"\n",
    "\n",
    "# Add the project root to Python path so imports like backend.* work\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Import the module and reload it to apply the latest file changes\n",
    "import backend.ollama_client as oc\n",
    "importlib.reload(oc)\n",
    "\n",
    "# Print available generation functions to confirm they exist\n",
    "print(\"Functions found:\", [x for x in dir(oc) if \"ollama_generate\" in x])\n",
    "\n",
    "# Test the helper function and print the result\n",
    "test_prompt = \"اكتب كلمة واحدة فقط بدون أي شرح: مرحبا\"\n",
    "print(oc.ollama_generate_one_word_ar(test_prompt, model=\"llama3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe022219-1626-4025-a9f4-5d618c100923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected DOCX path: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\data\\raw\\dhofar flavor.docx\n",
      "Raw folder exists: True\n",
      "Files inside data/raw: ['dhofar flavor.docx', 'dhofar flavor.pdf']\n",
      "DOCX loaded. Characters: 41906\n",
      "Preview (first 800 chars):\n",
      "شوربة الجريش\n",
      "ID: DF-001\n",
      "نوع الأكلة: شوربة لحم\n",
      "المنطقة: محافظة ظفار\n",
      "طريقة الطهي: سلق / طبخ بطيء\n",
      "الوصف:\n",
      "تُعد شوربة الجريش من الأكلات التقليدية المعروفة في محافظة ظفار، وتُؤكل غالبًا كوجبة يومية دافئة، خاصة في الأجواء الباردة أو في المساء، ولا ترتبط بمناسبة محددة، لكنها حاضرة بشكل متكرر في البيوت لما تتميز به من قيمة غذائية عالية وقوام مشبع. تُحضَّر من الجريش المطبوخ مع اللحم والسمن والتوابل مثل القرفة واللومي اليابس، مما يمنحها نكهة غنية ومميّزة. تُقدَّم شوربة الجريش عادةً ساخنة، ويمكن تناولها بمفردها أو مع الخبز، وأحيانًا تُقدَّم إلى جانب أطباق خفيفة أخرى، وتعكس بساطة المطبخ الظفاري واعتماده على المكونات المحلية المتوفرة.\n",
      "المكونات:\n",
      "2 رأس بصل متوسط الحجم مفروم\n",
      "نصف كيلو لحم بدون عظم، مقطع مكعبات\n",
      "6 حبات لومي يابس مقشّر\n",
      "ملح حسب الرغبة\n",
      "1 كوب جريش\n",
      "3 أعواد قرفة\n",
      "ربع كوب سمن\n",
      "6 أكواب ماء\n",
      "طريقة التحضي\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\load_docx.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from docx import Document\n",
    "\n",
    "# Define the project root directory\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Define the raw data folder and expected DOCX file path\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DOCX_PATH = RAW_DIR / \"dhofar flavor.docx\"\n",
    "\n",
    "# Define where the reusable loader script will be saved\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"load_docx.py\"\n",
    "\n",
    "def read_docx_text(docx_path: str) -> str:\n",
    "    # Read a DOCX file and return non-empty paragraphs as a single string\n",
    "    doc = Document(docx_path)\n",
    "    parts = []\n",
    "    for p in doc.paragraphs:\n",
    "        t = (p.text or \"\").strip()\n",
    "        if t:\n",
    "            parts.append(t)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# Show expected file location and raw folder status\n",
    "print(\"Expected DOCX path:\", DOCX_PATH)\n",
    "print(\"Raw folder exists:\", RAW_DIR.exists())\n",
    "\n",
    "# List files inside data/raw to help debugging\n",
    "if RAW_DIR.exists():\n",
    "    print(\"Files inside data/raw:\", [x.name for x in RAW_DIR.iterdir()])\n",
    "\n",
    "# Run the reader now in Jupyter and print a preview\n",
    "if not DOCX_PATH.exists():\n",
    "    print(\"DOCX not found. Please move or rename the file.\")\n",
    "    raw_text = \"\"\n",
    "else:\n",
    "    raw_text = read_docx_text(str(DOCX_PATH))\n",
    "    print(\"DOCX loaded. Characters:\", len(raw_text))\n",
    "    print(\"Preview (first 800 chars):\")\n",
    "    print(raw_text[:800])\n",
    "\n",
    "# Create the backend folder if needed and save the same logic as a Python file\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = f\"\"\"\\\n",
    "# Read domain text from a DOCX file (used as a knowledge source for RAG)\n",
    "\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "\n",
    "# Define the project root directory\n",
    "PROJECT_ROOT = Path(r\"{str(PROJECT_ROOT)}\")\n",
    "\n",
    "# Define the path to the domain DOCX file\n",
    "DOCX_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"dhofar flavor.docx\"\n",
    "\n",
    "def read_docx_text(docx_path: str) -> str:\n",
    "    # Read a DOCX file and return non-empty paragraphs as a single string\n",
    "    doc = Document(docx_path)\n",
    "    parts = []\n",
    "    for p in doc.paragraphs:\n",
    "        t = (p.text or \"\").strip()\n",
    "        if t:\n",
    "            parts.append(t)\n",
    "    return \"\\\\n\".join(parts)\n",
    "\n",
    "# Run a small test when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    if not DOCX_PATH.exists():\n",
    "        print(\"DOCX not found at:\", DOCX_PATH)\n",
    "    else:\n",
    "        raw_text = read_docx_text(str(DOCX_PATH))\n",
    "        print(\"DOCX loaded. Characters:\", len(raw_text))\n",
    "        print(raw_text[:800])\n",
    "\"\"\"\n",
    "\n",
    "# Write the file to disk\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "\n",
    "# Confirm that the file was saved\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e235316c-fcc7-44a9-83a8-4e6085c1e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes found: 45\n",
      "Sample preview:\n",
      "شوربة الجريش\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\chunking.py\n"
     ]
    }
   ],
   "source": [
    "# This cell does two things:\n",
    "# 1) Runs the recipe chunking now and prints a preview in Jupyter\n",
    "# 2) Saves the same logic into backend/chunking.py at the end\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Define the project root directory\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Define where the chunking module will be saved\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"chunking.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def split_recipes_docx(text: str):\n",
    "    # Split text into recipe blocks using the DF-ID markers\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    blocks = []\n",
    "    current = []\n",
    "\n",
    "    id_pattern = re.compile(r\"^ID\\s*:\\s*DF-\\S+\", re.IGNORECASE)\n",
    "\n",
    "    for line in lines:\n",
    "        if id_pattern.match(line):\n",
    "            # Save the previous block when a new ID starts\n",
    "            if current:\n",
    "                blocks.append(\"\\n\".join(current).strip())\n",
    "                current = []\n",
    "            current.append(line)\n",
    "        else:\n",
    "            # Append normal lines to the current block\n",
    "            current.append(line)\n",
    "\n",
    "    # Save the last block\n",
    "    if current:\n",
    "        blocks.append(\"\\n\".join(current).strip())\n",
    "\n",
    "    # Fix ordering if the ID line appears before the recipe name\n",
    "    fixed_blocks = []\n",
    "    for b in blocks:\n",
    "        b_lines = b.splitlines()\n",
    "        if len(b_lines) >= 2 and id_pattern.match(b_lines[0]) and not id_pattern.match(b_lines[1]):\n",
    "            b_lines = [b_lines[1], b_lines[0]] + b_lines[2:]\n",
    "        fixed_blocks.append(\"\\n\".join(b_lines))\n",
    "\n",
    "    return fixed_blocks\n",
    "\n",
    "# Run chunking now in Jupyter (raw_text must already exist from the DOCX loader cell)\n",
    "blocks = split_recipes_docx(raw_text)\n",
    "print(\"Recipes found:\", len(blocks))\n",
    "print(\"Sample preview:\")\n",
    "print(blocks[0][:800] if blocks else \"No blocks found\")\n",
    "\n",
    "# Save the same code into backend/chunking.py\n",
    "code_to_save = f\"\"\"\\\n",
    "# Chunking utilities for splitting domain text into recipe blocks\n",
    "\n",
    "import re\n",
    "\n",
    "def split_recipes_docx(text: str):\n",
    "    # Split text into recipe blocks using the DF-ID markers\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    blocks = []\n",
    "    current = []\n",
    "\n",
    "    id_pattern = re.compile(r\"^ID\\\\s*:\\\\s*DF-\\\\S+\", re.IGNORECASE)\n",
    "\n",
    "    for line in lines:\n",
    "        if id_pattern.match(line):\n",
    "            # Save the previous block when a new ID starts\n",
    "            if current:\n",
    "                blocks.append(\"\\\\n\".join(current).strip())\n",
    "                current = []\n",
    "            current.append(line)\n",
    "        else:\n",
    "            # Append normal lines to the current block\n",
    "            current.append(line)\n",
    "\n",
    "    # Save the last block\n",
    "    if current:\n",
    "        blocks.append(\"\\\\n\".join(current).strip())\n",
    "\n",
    "    # Fix ordering if the ID line appears before the recipe name\n",
    "    fixed_blocks = []\n",
    "    for b in blocks:\n",
    "        b_lines = b.splitlines()\n",
    "        if len(b_lines) >= 2 and id_pattern.match(b_lines[0]) and not id_pattern.match(b_lines[1]):\n",
    "            b_lines = [b_lines[1], b_lines[0]] + b_lines[2:]\n",
    "        fixed_blocks.append(\"\\\\n\".join(b_lines))\n",
    "\n",
    "    return fixed_blocks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Minimal self-test\n",
    "    sample = \"Dish Name\\\\nID: DF-001\\\\nIngredients: ...\\\\n\"\n",
    "    out = split_recipes_docx(sample)\n",
    "    print(\"Blocks:\", len(out))\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90dee7cd-88c4-4cd4-8411-95336a661431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed recipes: 44\n",
      "First recipe ID: DF-001\n",
      "First recipe name: شوربة الجريش\n",
      "Ingredients sample: ['2 رأس بصل متوسط الحجم مفروم', 'نصف كيلو لحم بدون عظم، مقطع مكعبات', '6 حبات لومي يابس مقشّر']\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\parser.py\n"
     ]
    }
   ],
   "source": [
    "# This cell does two things:\n",
    "# 1) Runs the DOCX parsing now and prints a preview in Jupyter\n",
    "# 2) Saves the same logic into backend/parser.py at the end\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Define the project root directory\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Define where the parser module will be saved\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"parser.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ================== NORMALIZATION ==================\n",
    "def normalize_ar(text: str) -> str:\n",
    "    # Normalize Arabic text for internal matching and keywords\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "# ================== PARSE RECIPES ==================\n",
    "def parse_recipes_from_docx_text(raw_text: str):\n",
    "    # Parse structured recipes from raw DOCX text using DF IDs and section headers\n",
    "    lines = [l.strip() for l in raw_text.splitlines() if l.strip()]\n",
    "    id_pat = re.compile(r\"^ID\\s*:\\s*(DF-[A-Za-z0-9\\-]+)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "    recipes = []\n",
    "    cur = None\n",
    "    section = None\n",
    "    last_text_line = None\n",
    "\n",
    "    def new_recipe():\n",
    "        # Create a new recipe object\n",
    "        return {\n",
    "            \"id\": None,\n",
    "            \"name\": None,\n",
    "            \"type\": None,\n",
    "            \"region\": None,\n",
    "            \"cook_method\": None,\n",
    "            \"description\": \"\",\n",
    "            \"ingredients\": [],\n",
    "            \"prep\": \"\",\n",
    "            \"keywords\": []\n",
    "        }\n",
    "\n",
    "    def flush():\n",
    "        # Finalize and store the current recipe if it has an ID\n",
    "        nonlocal cur\n",
    "        if not cur:\n",
    "            return\n",
    "        cur[\"description\"] = cur[\"description\"].strip()\n",
    "        cur[\"prep\"] = cur[\"prep\"].strip()\n",
    "\n",
    "        cur[\"keywords\"] = list(set(\n",
    "            normalize_ar(w)\n",
    "            for w in ((cur[\"name\"] or \"\").split() + (cur[\"type\"] or \"\").split())\n",
    "            if w\n",
    "        ))\n",
    "\n",
    "        if cur.get(\"id\"):\n",
    "            recipes.append(cur)\n",
    "        cur = None\n",
    "\n",
    "    for line in lines:\n",
    "        m = id_pat.match(line)\n",
    "        if m:\n",
    "            flush()\n",
    "            cur = new_recipe()\n",
    "            cur[\"id\"] = m.group(1).strip()\n",
    "\n",
    "            if last_text_line and \":\" not in last_text_line:\n",
    "                cur[\"name\"] = last_text_line.strip()\n",
    "            else:\n",
    "                cur[\"name\"] = None\n",
    "\n",
    "            section = None\n",
    "            continue\n",
    "\n",
    "        if not line.startswith((\"نوع الأكلة\", \"المنطقة\", \"طريقة الطهي\", \"الوصف\", \"المكونات\", \"طريقة التحضير\")):\n",
    "            if len(line) <= 60:\n",
    "                last_text_line = line\n",
    "\n",
    "        if not cur:\n",
    "            continue\n",
    "\n",
    "        if \"نوع الأكلة\" in line:\n",
    "            cur[\"type\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "        if \"المنطقة\" in line:\n",
    "            cur[\"region\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "        if \"طريقة الطهي\" in line:\n",
    "            cur[\"cook_method\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"الوصف\"):\n",
    "            section = \"description\"\n",
    "            continue\n",
    "        if line.startswith(\"المكونات\"):\n",
    "            section = \"ingredients\"\n",
    "            continue\n",
    "        if line.startswith(\"طريقة التحضير\"):\n",
    "            section = \"prep\"\n",
    "            continue\n",
    "\n",
    "        if section == \"description\":\n",
    "            cur[\"description\"] += line + \" \"\n",
    "        elif section == \"ingredients\":\n",
    "            if \":\" not in line and not line.startswith((\"ملاحظات\", \"تكفي\")):\n",
    "                cur[\"ingredients\"].append(line.strip())\n",
    "        elif section == \"prep\":\n",
    "            cur[\"prep\"] += line + \" \"\n",
    "\n",
    "    flush()\n",
    "    return recipes\n",
    "\n",
    "# ================== RUN NOW IN JUPYTER ==================\n",
    "recipes = parse_recipes_from_docx_text(raw_text)\n",
    "print(\"Parsed recipes:\", len(recipes))\n",
    "if recipes:\n",
    "    print(\"First recipe ID:\", recipes[0].get(\"id\"))\n",
    "    print(\"First recipe name:\", recipes[0].get(\"name\"))\n",
    "    print(\"Ingredients sample:\", (recipes[0].get(\"ingredients\") or [])[:3])\n",
    "else:\n",
    "    print(\"No recipes parsed. Check the DOCX format and ID lines.\")\n",
    "\n",
    "# ================== SAVE MODULE FILE ==================\n",
    "code_to_save = f\"\"\"\\\n",
    "# Parse recipes from DOCX text into structured objects\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    # Normalize Arabic text for internal matching and keywords\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\\\u064B-\\\\u065F\\\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    text = re.sub(r\"\\\\s+\",\" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def parse_recipes_from_docx_text(raw_text: str):\n",
    "    # Parse structured recipes from raw text using DF IDs and section headers\n",
    "    lines = [l.strip() for l in raw_text.splitlines() if l.strip()]\n",
    "    id_pat = re.compile(r\"^ID\\\\s*:\\\\s*(DF-[A-Za-z0-9\\\\-]+)\\\\s*$\", re.IGNORECASE)\n",
    "\n",
    "    recipes = []\n",
    "    cur = None\n",
    "    section = None\n",
    "    last_text_line = None\n",
    "\n",
    "    def new_recipe():\n",
    "        # Create a new recipe object\n",
    "        return {{\n",
    "            \"id\": None,\n",
    "            \"name\": None,\n",
    "            \"type\": None,\n",
    "            \"region\": None,\n",
    "            \"cook_method\": None,\n",
    "            \"description\": \"\",\n",
    "            \"ingredients\": [],\n",
    "            \"prep\": \"\",\n",
    "            \"keywords\": []\n",
    "        }}\n",
    "\n",
    "    def flush():\n",
    "        # Finalize and store the current recipe if it has an ID\n",
    "        nonlocal cur\n",
    "        if not cur:\n",
    "            return\n",
    "        cur[\"description\"] = cur[\"description\"].strip()\n",
    "        cur[\"prep\"] = cur[\"prep\"].strip()\n",
    "        cur[\"keywords\"] = list(set(\n",
    "            normalize_ar(w)\n",
    "            for w in ((cur[\"name\"] or \"\").split() + (cur[\"type\"] or \"\").split())\n",
    "            if w\n",
    "        ))\n",
    "        if cur.get(\"id\"):\n",
    "            recipes.append(cur)\n",
    "        cur = None\n",
    "\n",
    "    for line in lines:\n",
    "        m = id_pat.match(line)\n",
    "        if m:\n",
    "            flush()\n",
    "            cur = new_recipe()\n",
    "            cur[\"id\"] = m.group(1).strip()\n",
    "            if last_text_line and \":\" not in last_text_line:\n",
    "                cur[\"name\"] = last_text_line.strip()\n",
    "            else:\n",
    "                cur[\"name\"] = None\n",
    "            section = None\n",
    "            continue\n",
    "\n",
    "        if not line.startswith((\"نوع الأكلة\", \"المنطقة\", \"طريقة الطهي\", \"الوصف\", \"المكونات\", \"طريقة التحضير\")):\n",
    "            if len(line) <= 60:\n",
    "                last_text_line = line\n",
    "\n",
    "        if not cur:\n",
    "            continue\n",
    "\n",
    "        if \"نوع الأكلة\" in line:\n",
    "            cur[\"type\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "        if \"المنطقة\" in line:\n",
    "            cur[\"region\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "        if \"طريقة الطهي\" in line:\n",
    "            cur[\"cook_method\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"الوصف\"):\n",
    "            section = \"description\"\n",
    "            continue\n",
    "        if line.startswith(\"المكونات\"):\n",
    "            section = \"ingredients\"\n",
    "            continue\n",
    "        if line.startswith(\"طريقة التحضير\"):\n",
    "            section = \"prep\"\n",
    "            continue\n",
    "\n",
    "        if section == \"description\":\n",
    "            cur[\"description\"] += line + \" \"\n",
    "        elif section == \"ingredients\":\n",
    "            if \":\" not in line and not line.startswith((\"ملاحظات\", \"تكفي\")):\n",
    "                cur[\"ingredients\"].append(line.strip())\n",
    "        elif section == \"prep\":\n",
    "            cur[\"prep\"] += line + \" \"\n",
    "\n",
    "    flush()\n",
    "    return recipes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Minimal self-test\n",
    "    sample = \"اسم\\\\nID: DF-001\\\\nنوع الأكلة: ...\\\\nالمكونات\\\\nسكر\\\\nطريقة التحضير\\\\nخطوة\\\\n\"\n",
    "    out = parse_recipes_from_docx_text(sample)\n",
    "    print(\"Parsed:\", len(out))\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88d5b594-a8e7-4b72-bc92-e69d3f0b36eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF-001 شوربة الجريش\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0][\"id\"], recipes[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c579820e-1f26-4ed4-b19d-a728b87f2938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique keywords: 110\n",
      "Test: شوربة => [{'id': 'DF-001', 'name': 'شوربة الجريش'}]\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\keyword_index.py\n"
     ]
    }
   ],
   "source": [
    "# This cell does two things:\n",
    "# 1) Builds the keyword-to-recipes index now and prints a preview in Jupyter\n",
    "# 2) Saves the same logic into backend/keyword_index.py at the end\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Ensure recipes exist (must be created by the parser cell)\n",
    "assert \"recipes\" in globals(), \"Variable 'recipes' is not defined. Run the parsing cell first.\"\n",
    "\n",
    "# Define the project root directory\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Define where the keyword index module will be saved\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"keyword_index.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    # Normalize Arabic text for keyword matching\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "# Build keyword → recipes index\n",
    "keyword_to_recipes = defaultdict(list)\n",
    "\n",
    "for r in recipes:\n",
    "    for kw in r.get(\"keywords\", []):\n",
    "        k = normalize_ar(kw)\n",
    "        if k:\n",
    "            keyword_to_recipes[k].append({\n",
    "                \"id\": r.get(\"id\"),\n",
    "                \"name\": r.get(\"name\")\n",
    "            })\n",
    "\n",
    "# Show results now in Jupyter\n",
    "print(\"Unique keywords:\", len(keyword_to_recipes))\n",
    "\n",
    "test_kw = \"شوربة\"\n",
    "print(\"Test:\", test_kw, \"=>\",\n",
    "      keyword_to_recipes.get(normalize_ar(test_kw), [])[:5])\n",
    "\n",
    "# Save the same logic into backend/keyword_index.py\n",
    "code_to_save = \"\"\"\\\n",
    "# Build a keyword-to-recipes index for fast lookup\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    # Normalize Arabic text for keyword matching\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\\\u064B-\\\\u065F\\\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    text = re.sub(r\"\\\\s+\",\" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def build_keyword_index(recipes):\n",
    "    # Create a mapping from keyword to related recipes\n",
    "    keyword_to_recipes = defaultdict(list)\n",
    "\n",
    "    for r in recipes:\n",
    "        for kw in r.get(\"keywords\", []):\n",
    "            k = normalize_ar(kw)\n",
    "            if k:\n",
    "                keyword_to_recipes[k].append({\n",
    "                    \"id\": r.get(\"id\"),\n",
    "                    \"name\": r.get(\"name\")\n",
    "                })\n",
    "\n",
    "    return keyword_to_recipes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Minimal self-test\n",
    "    sample = [{\n",
    "        \"id\": \"DF-001\",\n",
    "        \"name\": \"Test Dish\",\n",
    "        \"keywords\": [\"شوربة\", \"تقليدي\"]\n",
    "    }]\n",
    "    idx = build_keyword_index(sample)\n",
    "    print(\"Keywords:\", list(idx.keys()))\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11973d7e-7b86-42e2-a6d1-6cb3b11ca361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أكلات تحتوي (سمك) في المكونات:\n",
      "1) مرق السمك بالنارجيل — (DF-014)\n",
      "2) عطراية (الحبار) — (DF-016)\n",
      "3) الربيس — (DF-017)\n",
      "4) الصيادية — (DF-018)\n",
      "5) المضبي — (DF-023)\n",
      "6) المالح — (DF-036)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "----\n",
      "أكلات تحتوي (لحم) في المكونات:\n",
      "1) شوربة الجريش — (DF-001)\n",
      "2) سوب اللحم والطماطم — (DF-002)\n",
      "3) كمباه مقشّد — (DF-008)\n",
      "4) الجريــش باللحم — (DF-012)\n",
      "5) رز مقزّح — (DF-013)\n",
      "6) لحم مفور — (DF-022)\n",
      "7) المضبي — (DF-023)\n",
      "8) المقديد — (DF-024)\n",
      "9) المعجين — (DF-025)\n",
      "10) قبولي — (DF-040)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\keyword_router.py\n"
     ]
    }
   ],
   "source": [
    "# ================== STRICT KEYWORD MATCHING (INGREDIENTS ONLY) ==================\n",
    "# This cell:\n",
    "# 1) Runs the keyword search now and prints results in Jupyter\n",
    "# 2) Saves the same code into backend/keyword_router.py at the end\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ================== ORIGINAL CODE (UNCHANGED) ==================\n",
    "def keyword_suggest(query: str, top_k: int = 12):\n",
    "    q = normalize_ar(query)\n",
    "    if not q:\n",
    "        return [], \"none\"\n",
    "\n",
    "    # Strict search inside ingredients only\n",
    "    pattern = re.compile(rf\"(^|\\s){re.escape(q)}(\\s|$)\")\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = normalize_ar(\" \".join(r.get(\"ingredients\", [])))\n",
    "        if pattern.search(ing_text):\n",
    "            out.append({\"id\": r[\"id\"], \"name\": r[\"name\"]})\n",
    "\n",
    "    # Remove duplicates\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for it in out:\n",
    "        key = (it[\"id\"], it[\"name\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(it)\n",
    "\n",
    "    return uniq[:top_k], \"direct\" if uniq else \"none\"\n",
    "\n",
    "def show_keyword_results(query: str):\n",
    "    items, mode = keyword_suggest(query, top_k=12)\n",
    "    if not items:\n",
    "        return \"ما لقيت أكلات تحتوي هذه الكلمة داخل المكونات.\"\n",
    "    text = f\"أكلات تحتوي ({query}) في المكونات:\\n\"\n",
    "    for i, it in enumerate(items, 1):\n",
    "        text += f\"{i}) {it['name']} — ({it['id']})\\n\"\n",
    "    text += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "    return text\n",
    "\n",
    "# ================== RUN NOW (JUPYTER OUTPUT) ==================\n",
    "print(show_keyword_results(\"سمك\"))\n",
    "print(\"----\")\n",
    "print(show_keyword_results(\"لحم\"))\n",
    "\n",
    "# ================== SAVE SAME CODE TO FILE ==================\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"keyword_router.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "import re\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\\\u064B-\\\\u065F\\\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    text = re.sub(r\"\\\\s+\",\" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def keyword_suggest(query: str, recipes, top_k: int = 12):\n",
    "    q = normalize_ar(query)\n",
    "    if not q:\n",
    "        return [], \"none\"\n",
    "\n",
    "    pattern = re.compile(rf\"(^|\\\\s){re.escape(q)}(\\\\s|$)\")\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = normalize_ar(\" \".join(r.get(\"ingredients\", [])))\n",
    "        if pattern.search(ing_text):\n",
    "            out.append({\"id\": r[\"id\"], \"name\": r[\"name\"]})\n",
    "\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for it in out:\n",
    "        key = (it[\"id\"], it[\"name\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(it)\n",
    "\n",
    "    return uniq[:top_k], \"direct\" if uniq else \"none\"\n",
    "\n",
    "def show_keyword_results(query: str, recipes):\n",
    "    items, mode = keyword_suggest(query, recipes, top_k=12)\n",
    "    if not items:\n",
    "        return \"ما لقيت أكلات تحتوي هذه الكلمة داخل المكونات.\"\n",
    "    text = f\"أكلات تحتوي ({query}) في المكونات:\\\\n\"\n",
    "    for i, it in enumerate(items, 1):\n",
    "        text += f\"{i}) {it['name']} — ({it['id']})\\\\n\"\n",
    "    text += \"\\\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "    return text\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67cb7842-daca-4f7d-b3af-d16130fae706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e59a92f5e44835870ed07718c12ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ready | vectors: 44 | dim: 384\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\vectorstore_faiss.py\n"
     ]
    }
   ],
   "source": [
    "# ================== EMBEDDINGS + FAISS (RUN + SAVE) ==================\n",
    "# This cell:\n",
    "# 1) Builds embeddings and FAISS index now and prints output in Jupyter\n",
    "# 2) Saves the same logic into backend/vectorstore_faiss.py (single file)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ================== RUN NOW (JUPYTER OUTPUT) ==================\n",
    "\n",
    "# Define embedding model\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "# Prepare documents for vector database\n",
    "texts = []\n",
    "for r in recipes:\n",
    "    doc = (\n",
    "        f\"اسم: {r['name']}\\n\"\n",
    "        f\"ID: {r['id']}\\n\"\n",
    "        f\"نوع: {r.get('type','')}\\n\"\n",
    "        f\"المنطقة: {r.get('region','')}\\n\"\n",
    "        f\"طريقة الطهي: {r.get('cook_method','')}\\n\"\n",
    "        f\"الوصف: {r.get('description','')}\\n\"\n",
    "        f\"المكونات: {', '.join(r.get('ingredients', [])[:20])}\\n\"\n",
    "        f\"الكلمات المفتاحية: {', '.join(r.get('keywords', []))}\\n\"\n",
    "        f\"طريقة التحضير: {r.get('prep','')}\"\n",
    "    )\n",
    "    texts.append(doc)\n",
    "\n",
    "# Generate embeddings\n",
    "emb = embedder.encode(texts, show_progress_bar=True)\n",
    "emb = np.array(emb, dtype=np.float32)\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "# Print result in Jupyter\n",
    "print(\"FAISS ready | vectors:\", index.ntotal, \"| dim:\", emb.shape[1])\n",
    "\n",
    "# ================== SAVE SAME CODE TO FILE ==================\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"vectorstore_faiss.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "# Build embeddings and FAISS vector database for recipe documents\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "def build_faiss_index(recipes):\n",
    "    # Load embedding model\n",
    "    embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "    # Prepare documents for vector database\n",
    "    texts = []\n",
    "    for r in recipes:\n",
    "        doc = (\n",
    "            f\"اسم: {r['name']}\\\\n\"\n",
    "            f\"ID: {r['id']}\\\\n\"\n",
    "            f\"نوع: {r.get('type','')}\\\\n\"\n",
    "            f\"المنطقة: {r.get('region','')}\\\\n\"\n",
    "            f\"طريقة الطهي: {r.get('cook_method','')}\\\\n\"\n",
    "            f\"الوصف: {r.get('description','')}\\\\n\"\n",
    "            f\"المكونات: {', '.join(r.get('ingredients', [])[:20])}\\\\n\"\n",
    "            f\"الكلمات المفتاحية: {', '.join(r.get('keywords', []))}\\\\n\"\n",
    "            f\"طريقة التحضير: {r.get('prep','')}\"\n",
    "        )\n",
    "        texts.append(doc)\n",
    "\n",
    "    # Generate embeddings\n",
    "    emb = embedder.encode(texts, show_progress_bar=True)\n",
    "    emb = np.array(emb, dtype=np.float32)\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    index.add(emb)\n",
    "\n",
    "    return index, texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Minimal self-test\n",
    "    sample = [{\n",
    "        \"id\": \"DF-001\",\n",
    "        \"name\": \"Test Dish\",\n",
    "        \"type\": \"Traditional\",\n",
    "        \"region\": \"Dhofar\",\n",
    "        \"cook_method\": \"Boiling\",\n",
    "        \"description\": \"Test description\",\n",
    "        \"ingredients\": [\"لحم\", \"ملح\"],\n",
    "        \"keywords\": [\"تقليدي\"],\n",
    "        \"prep\": \"Test steps\"\n",
    "    }]\n",
    "    idx, docs = build_faiss_index(sample)\n",
    "    print(\"FAISS vectors:\", idx.ntotal)\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46fc75bd-a457-4773-9237-9166616bc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval (RAG) + Ollama Generate (Domain-Restricted)\n",
    "\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "def retrieve_top_chunks(query: str, top_k: int = 4):\n",
    "    # Convert the user query into an embedding vector\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "\n",
    "    # Search the FAISS index to get the nearest recipe documents\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "\n",
    "    # Collect the matched recipe texts using their indices\n",
    "    chunks = []\n",
    "    for idx in I[0]:\n",
    "        chunks.append(texts[idx])\n",
    "\n",
    "    # Return the retrieved chunks as context for the LLM\n",
    "    return chunks\n",
    "\n",
    "def ollama_generate(prompt: str, model=\"llama3\"):\n",
    "    # Send the prompt to Ollama and return the generated response text\n",
    "    r = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    return (r.stdout or \"\").strip()\n",
    "\n",
    "# A domain warning to remind the assistant to answer only from the provided documents\n",
    "DOMAIN_DISCLAIMER = (\n",
    "    \"تنبيه: هذا الشات يجيب فقط من وثائق (وصفات ظفار) داخل المشروع. \"\n",
    "    \"إذا المعلومة غير موجودة في الوثائق، سيتم إرجاع: غير موجود في البيانات.\"\n",
    ")\n",
    "\n",
    "def rag_answer(user_q: str, model=\"llama3\"):\n",
    "    # Retrieve top relevant recipe chunks for the question\n",
    "    chunks = retrieve_top_chunks(user_q, top_k=4)\n",
    "\n",
    "    # Combine chunks into one context string\n",
    "    context = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "\n",
    "    # Build a strict prompt that forces answers from context only\n",
    "    prompt = f\"\"\"\n",
    "{DOMAIN_DISCLAIMER}\n",
    "قواعد مهمة:\n",
    "- استخدم (السياق) فقط.\n",
    "- إذا لم تجد الإجابة في السياق، اكتب بالضبط: غير موجود في البيانات\n",
    "- لا تخمّن ولا تضف معلومات خارج السياق.\n",
    "السياق:\n",
    "{context}\n",
    "سؤال المستخدم:\n",
    "{user_q}\n",
    "الإجابة:\n",
    "\"\"\"\n",
    "\n",
    "    # Generate the final answer using the LLM\n",
    "    return ollama_generate(prompt, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7e4172e-71c4-4953-8896-b7ee09cec32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines a \"smart router\" for your chatbot:\n",
    "# It detects what the user wants (ingredients/prep/description/all), finds recipes by name,\n",
    "# suggests recipes by ingredient keyword (strict), handles number selection, and falls back to RAG for long questions.\n",
    "\n",
    "import re  # Regular expressions for strict word matching\n",
    "\n",
    "# ---------- 1) Intent Detection (decide what the user is asking for) ----------\n",
    "def detect_intent(user_text: str) -> str:\n",
    "    t = normalize_ar(user_text)  # Normalize the user text for easier matching\n",
    "\n",
    "    wants_ing = any(w in t for w in [\"مكونات\", \"مقادير\", \"المكونات\", \"ingredients\"])  # Check if user wants ingredients\n",
    "    wants_prep = any(w in t for w in [\"طريقة\", \"تحضير\", \"اطبخ\", \"اسوي\", \"كيف\", \"خطوات\", \"prep\", \"cook\"])  # Check if user wants steps\n",
    "    wants_desc = any(w in t for w in [\"وصف\", \"نبذه\", \"نبذة\", \"تعريف\", \"description\", \"about\"])  # Check if user wants description\n",
    "    wants_all  = any(w in t for w in [\"كل\", \"كامل\", \"كامله\", \"كامله\", \"everything\", \"all\"])  # Check if user wants everything\n",
    "\n",
    "    # Use a clear priority order\n",
    "    if wants_all:  # If user asked for everything\n",
    "        return \"all\"\n",
    "    if wants_ing and wants_prep and wants_desc:  # If user asked for all parts together\n",
    "        return \"all\"\n",
    "    if wants_ing:  # If user asked for ingredients\n",
    "        return \"ingredients\"\n",
    "    if wants_prep:  # If user asked for preparation steps\n",
    "        return \"prep\"\n",
    "    if wants_desc:  # If user asked for description\n",
    "        return \"description\"\n",
    "\n",
    "    # Default: when dish name is mentioned without a clear intent, show everything\n",
    "    return \"all\"\n",
    "\n",
    "\n",
    "# ---------- 2) Formatting (show only the requested part) ----------\n",
    "def format_recipe_part(r: dict, intent: str) -> str:\n",
    "    header = f\"{r['name']} — ({r['id']})\\n\"  # Title line with recipe name and ID\n",
    "\n",
    "    meta = \"\"  # Meta info block (type/region/cook method)\n",
    "    if r.get(\"type\"): \n",
    "        meta += f\"نوع الأكلة: {r['type']}\\n\"  # Add recipe type if available\n",
    "    if r.get(\"region\"): \n",
    "        meta += f\"المنطقة: {r['region']}\\n\"  # Add region if available\n",
    "    if r.get(\"cook_method\"): \n",
    "        meta += f\"طريقة الطهي: {r['cook_method']}\\n\"  # Add cooking method if available\n",
    "\n",
    "    if intent == \"description\":  # If user wants description only\n",
    "        body = f\"\\nالوصف:\\n{r.get('description','').strip()}\" if r.get(\"description\") else \"\\nالوصف:\\nغير موجود في البيانات\"\n",
    "        return (header + meta + body).strip()\n",
    "\n",
    "    if intent == \"ingredients\":  # If user wants ingredients only\n",
    "        if r.get(\"ingredients\"):\n",
    "            ing = \"\\n\".join([f\"- {x}\" for x in r[\"ingredients\"]])  # Convert ingredient list into bullet lines\n",
    "            return (header + meta + \"\\nالمكونات:\\n\" + ing).strip()\n",
    "        return (header + meta + \"\\nالمكونات:\\nغير موجود في البيانات\").strip()\n",
    "\n",
    "    if intent == \"prep\":  # If user wants preparation steps only\n",
    "        body = f\"\\nطريقة التحضير:\\n{r.get('prep','').strip()}\" if r.get(\"prep\") else \"\\nطريقة التحضير:\\nغير موجود في البيانات\"\n",
    "        return (header + meta + body).strip()\n",
    "\n",
    "    # intent == \"all\" (show all parts)\n",
    "    parts = [header + meta]  # Start with header + meta\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        parts.append(\"الوصف:\\n\" + r[\"description\"].strip())  # Add description if available\n",
    "    else:\n",
    "        parts.append(\"الوصف:\\nغير موجود في البيانات\")  # Fallback if missing\n",
    "\n",
    "    if r.get(\"ingredients\"):\n",
    "        parts.append(\"المكونات:\\n\" + \"\\n\".join([f\"- {x}\" for x in r[\"ingredients\"]]))  # Add ingredients list\n",
    "    else:\n",
    "        parts.append(\"المكونات:\\nغير موجود في البيانات\")  # Fallback if missing\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        parts.append(\"طريقة التحضير:\\n\" + r[\"prep\"].strip())  # Add preparation steps\n",
    "    else:\n",
    "        parts.append(\"طريقة التحضير:\\nغير موجود في البيانات\")  # Fallback if missing\n",
    "\n",
    "    return \"\\n\\n\".join(parts).strip()  # Join sections with blank lines\n",
    "\n",
    "\n",
    "# ---------- 3) Find recipe by name (full or partial match) ----------\n",
    "def find_recipes_in_text(user_text: str, top_k: int = 5):\n",
    "    q = normalize_ar(user_text)  # Normalize the user query\n",
    "    matches = []  # Store (score, recipe) matches\n",
    "\n",
    "    for r in recipes:  # Loop through all recipes\n",
    "        name_n = normalize_ar(r[\"name\"])  # Normalize recipe name\n",
    "        if name_n and (name_n in q or q in name_n):  # Match if the name is inside the message or the message is inside the name\n",
    "            score = len(name_n)  # Longer name match usually means a more precise match\n",
    "            matches.append((score, r))  # Save the match\n",
    "\n",
    "    matches.sort(key=lambda x: x[0], reverse=True)  # Sort by score descending\n",
    "    return [r for _, r in matches[:top_k]]  # Return top matches only\n",
    "\n",
    "\n",
    "# ---------- 4) Strict keyword search in ingredients only ----------\n",
    "def suggest_recipes_by_term(term: str, top_k: int = 12):\n",
    "    t = normalize_ar(term)  # Normalize the keyword\n",
    "    if not t:\n",
    "        return []  # If empty after normalization, return nothing\n",
    "\n",
    "    pattern = re.compile(rf\"(^|\\s){re.escape(t)}(\\s|$)\")  # Strict whole-word match\n",
    "    results = []  # Store matching recipes (id + name)\n",
    "\n",
    "    for r in recipes:  # Loop through all recipes\n",
    "        ing_text = normalize_ar(\" \".join(r.get(\"ingredients\", [])))  # Join ingredients and normalize as one text\n",
    "        if pattern.search(ing_text):  # Check if keyword exists in ingredients only\n",
    "            results.append({\"id\": r[\"id\"], \"name\": r[\"name\"]})  # Add match\n",
    "\n",
    "    seen = set()  # Track duplicates\n",
    "    unique = []  # Store unique matches\n",
    "    for it in results:\n",
    "        key = (it[\"id\"], it[\"name\"])  # Unique key\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(it)\n",
    "\n",
    "    return unique[:top_k]  # Return top results\n",
    "\n",
    "\n",
    "# ---------- 5) Main dispatcher (decide what to do) ----------\n",
    "def smart_router(user_text: str, state: dict):\n",
    "    msg = user_text.strip()  # Remove extra spaces\n",
    "    msg_n = normalize_ar(msg)  # Normalize message (kept for future expansions)\n",
    "\n",
    "    # (A) If the user typed a number after ingredient suggestions\n",
    "    if msg.isdigit() and state.get(\"last_suggestions\"):\n",
    "        choice = int(msg)  # Convert to integer\n",
    "        opts = state[\"last_suggestions\"]  # Get last suggestions list\n",
    "        if 1 <= choice <= len(opts):\n",
    "            picked_id = opts[choice - 1][\"id\"]  # Get chosen recipe ID\n",
    "            picked = next((r for r in recipes if r[\"id\"] == picked_id), None)  # Find full recipe by ID\n",
    "            state[\"last_suggestions\"] = None  # Clear suggestions after choosing\n",
    "\n",
    "            if picked:\n",
    "                return format_recipe_part(picked, \"all\"), state  # Show full recipe\n",
    "            return \"ما قدرت أجيب تفاصيل الأكلة.\", state  # Fallback if not found\n",
    "        return f\"اختاري رقم بين 1 و {len(opts)}\", state  # Ask for a valid range\n",
    "\n",
    "    # (B) If the message contains a dish name (full or partial)\n",
    "    name_hits = find_recipes_in_text(msg, top_k=6)  # Find recipes by name in the message\n",
    "    if name_hits:\n",
    "        intent = detect_intent(msg)  # Detect user intent (ingredients/prep/description/all)\n",
    "\n",
    "        if len(name_hits) > 1:  # If multiple recipes match the name\n",
    "            state[\"last_name_options\"] = name_hits  # Save options for number selection\n",
    "            state[\"last_suggestions\"] = None  # Clear ingredient suggestions\n",
    "            out = \"لقيت أكثر من أكلة/نسخة مطابقة، اختاري رقم:\\n\\n\"\n",
    "            for i, r in enumerate(name_hits, 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الاختيار.\"\n",
    "            return out, state\n",
    "\n",
    "        state[\"last_name_options\"] = None  # Clear name options\n",
    "        state[\"last_suggestions\"] = None  # Clear suggestions\n",
    "        return format_recipe_part(name_hits[0], intent), state  # Show the matched recipe part\n",
    "\n",
    "    # (C) If the user typed a number after name options list\n",
    "    if msg.isdigit() and state.get(\"last_name_options\"):\n",
    "        choice = int(msg)  # Convert to integer\n",
    "        opts = state[\"last_name_options\"]  # Get the stored name options\n",
    "        if 1 <= choice <= len(opts):\n",
    "            picked = opts[choice - 1]  # Pick the selected recipe\n",
    "            state[\"last_name_options\"] = None  # Clear options\n",
    "            return format_recipe_part(picked, \"all\"), state  # Show full recipe\n",
    "        return f\"اختاري رقم بين 1 و {len(opts)}\", state  # Ask for a valid range\n",
    "\n",
    "    # (D) If the message is short, treat it as a keyword for ingredient search\n",
    "    if len(msg.split()) <= 2:\n",
    "        items = suggest_recipes_by_term(msg, top_k=12)  # Get suggestions by ingredient keyword\n",
    "        state[\"last_suggestions\"] = items if items else None  # Store suggestions for number selection\n",
    "        state[\"last_name_options\"] = None  # Clear name options\n",
    "        return render_suggestions(msg, items), state  # Show suggestions list (render_suggestions must exist)\n",
    "\n",
    "    # (E) If the message is long, use RAG + Ollama if available\n",
    "    state[\"last_suggestions\"] = None  # Clear suggestions\n",
    "    state[\"last_name_options\"] = None  # Clear name options\n",
    "\n",
    "    if \"rag_answer\" in globals():  # Check if RAG function exists in the notebook\n",
    "        return rag_answer(msg, model=MODEL_NAME), state  # Use RAG answer\n",
    "\n",
    "    return \"rag_answer غير مُعرّفة. شغّلي خلية RAG (FAISS + Ollama) أولاً.\", state  # Final fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aadeb262-bba4-43f5-b911-d786e1131a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\n",
      "\n",
      "1) أرز مع اللوبيا الحمراء ( رز ودجر) — (DF-028)\n",
      "2) سوب اللحم والطماطم — (DF-002)\n",
      "3) الجريــش باللحم — (DF-012)\n",
      "4) لحم مفور — (DF-022)\n",
      "5) الحمضوت — (DF-034)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "----\n",
      "أرز مع اللوبيا الحمراء ( رز ودجر) — (DF-028)\n",
      "نوع الأكلة: أرز + بقوليات / طبق رئيسي\n",
      "المنطقة: محافظة ظفار\n",
      "طريقة الطهي: طبخ بالأرز\n",
      "\n",
      "الوصف:\n",
      "رز ودجر من الأكلات اليومية الشائعة في البيوت الظفارية، خاصة في المناطق الريفية. يُحضَّر من الأرز واللوبيا الحمراء ويُقدَّم غالبًا مع السمن البلدي، وقد يُحضَّر أحيانًا في المناسبات الخاصة.\n",
      "\n",
      "المكونات:\n",
      "- 3  أكواب أرز بسمتي\n",
      "- 1.5 كوب دجر (لوبيا حمراء)\n",
      "- 3 بصلة كبيرة\n",
      "- 3 فصوص ثوم\n",
      "- 3 ملاعق كبيرة سمن\n",
      "- 1 ملعقة كبيرة ملح\n",
      "- 6 أكواب ماء\n",
      "\n",
      "طريقة التحضير:\n",
      "يُنقع الدجر في الماء لمدة أربع ساعات، ثم يُسلق نصف سلقة. يُفرم البصل ويُحمّر بالسمن حتى يذبل. يُضاف الثوم ويُقلّب قليلًا. يُضاف الدجر ويُقلّب مع البصل جيدًا. يُغسل الأرز ويُضاف إلى القدر. يُضاف الماء والملح. يُترك على نار متوسطة حتى ينضج الأرز تمامًا. تكفي كم شخص: من 4 إلى 6 أشخاص الكلمات المفتاحية: رز ودجر لوبيا حمراء أرز الارز سمن بلدي أكلة يومية مناسبات وجبة عشاء وجبة غداء بلاليط\n",
      "----\n",
      "لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\n",
      "\n",
      "1) شوربة الجريش — (DF-001)\n",
      "2) الجريش — (DF-020A)\n",
      "3) الجريش — (DF-020B)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\router.py\n"
     ]
    }
   ],
   "source": [
    "# This cell implements the final rule-based router for the chatbot.\n",
    "# It normalizes Arabic text, finds recipes by name or ingredient keyword,\n",
    "# formats recipe output, keeps conversation state, and falls back to RAG for long questions.\n",
    "# At the end, the same code is saved into backend/router.py\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ===================== NORMALIZATION =====================\n",
    "def normalize_ar(text: str) -> str:\n",
    "    # Normalize Arabic text to make matching consistent\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")  # Normalize Alef forms\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")  # Normalize Ya and Ta Marbuta\n",
    "    text = re.sub(r\"\\s+\",\" \", text).strip().lower()  # Remove extra spaces and lowercase\n",
    "    return text\n",
    "\n",
    "\n",
    "# ===================== FIND BY NAME (FULL/PARTIAL) =====================\n",
    "def find_recipes_by_name(user_text: str):\n",
    "    # Find recipes where the name matches the user text (full or partial)\n",
    "    q = normalize_ar(user_text)\n",
    "    hits = []\n",
    "    for r in recipes:\n",
    "        name_n = normalize_ar(r.get(\"name\",\"\"))\n",
    "        if not name_n:\n",
    "            continue\n",
    "        if q == name_n or q in name_n or name_n in q:\n",
    "            hits.append(r)\n",
    "\n",
    "    # Prefer longer (more specific) names\n",
    "    hits.sort(key=lambda x: len(normalize_ar(x.get(\"name\",\"\"))), reverse=True)\n",
    "    return hits\n",
    "\n",
    "\n",
    "# ===================== STRICT TERM IN INGREDIENTS ONLY =====================\n",
    "def find_recipes_by_term(term: str):\n",
    "    # Match keyword strictly inside ingredients only\n",
    "    t = normalize_ar(term)\n",
    "    if not t:\n",
    "        return []\n",
    "\n",
    "    pattern = re.compile(rf\"(^|\\s){re.escape(t)}(\\s|$)\")\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = normalize_ar(\" \".join(r.get(\"ingredients\", [])))\n",
    "        if pattern.search(ing_text):\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== FORMAT RECIPE (NO KEYWORDS SHOWN) =====================\n",
    "def format_recipe(r: dict) -> str:\n",
    "    # Convert a recipe dictionary into a readable response\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\n\"\n",
    "\n",
    "    if r.get(\"type\"):\n",
    "        out += f\"نوع الأكلة: {r['type']}\\n\"\n",
    "    if r.get(\"region\"):\n",
    "        out += f\"المنطقة: {r['region']}\\n\"\n",
    "    if r.get(\"cook_method\"):\n",
    "        out += f\"طريقة الطهي: {r['cook_method']}\\n\"\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        out += \"\\nالوصف:\\n\" + r[\"description\"].strip()\n",
    "\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\n\\nالمكونات:\\n\"\n",
    "        for ing in r[\"ingredients\"]:\n",
    "            out += f\"- {ing}\\n\"\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        out += \"\\nطريقة التحضير:\\n\" + r[\"prep\"].strip()\n",
    "\n",
    "    return out.strip()\n",
    "\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def is_short_term(msg: str) -> bool:\n",
    "    # One or two words are treated as a keyword search\n",
    "    return len(msg.split()) <= 2\n",
    "\n",
    "\n",
    "# ===================== STATE =====================\n",
    "STATE = {\n",
    "    \"options\": None,        # Last list of recipe options\n",
    "    \"options_mode\": None    # Source of options (name or term)\n",
    "}\n",
    "\n",
    "\n",
    "# ===================== MAIN ROUTER =====================\n",
    "def fixed_router(msg: str):\n",
    "    # Main decision function that routes user input\n",
    "    msg = (msg or \"\").strip()\n",
    "    if not msg:\n",
    "        return \"اكتبي كلمة مفتاحية (مثل: لحم/سمك) أو اسم أكلة أو سؤال طويل.\"\n",
    "\n",
    "    # 0) User selects a number from a previous list\n",
    "    if msg.isdigit() and STATE[\"options\"]:\n",
    "        idx = int(msg) - 1\n",
    "        opts = STATE[\"options\"]\n",
    "        if 0 <= idx < len(opts):\n",
    "            picked = opts[idx]\n",
    "            STATE[\"options\"] = None\n",
    "            STATE[\"options_mode\"] = None\n",
    "            return format_recipe(picked)\n",
    "        return \"اختاري رقم صحيح من القائمة.\"\n",
    "\n",
    "    # 1) Search by recipe name\n",
    "    name_matches = find_recipes_by_name(msg)\n",
    "    if name_matches:\n",
    "        if len(name_matches) == 1:\n",
    "            STATE[\"options\"] = None\n",
    "            STATE[\"options_mode\"] = None\n",
    "            return format_recipe(name_matches[0])\n",
    "\n",
    "        STATE[\"options\"] = name_matches\n",
    "        STATE[\"options_mode\"] = \"name\"\n",
    "        out = \"لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\\n\\n\"\n",
    "        for i, r in enumerate(name_matches, 1):\n",
    "            out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "        out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "        return out\n",
    "\n",
    "    # 2) Short keyword → strict ingredient search\n",
    "    if is_short_term(msg):\n",
    "        term_matches = find_recipes_by_term(msg)\n",
    "        if term_matches:\n",
    "            STATE[\"options\"] = term_matches\n",
    "            STATE[\"options_mode\"] = \"term\"\n",
    "            out = f\"أكلات تحتوي ({msg}) داخل المكونات:\\n\\n\"\n",
    "            for i, r in enumerate(term_matches, 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "        return \"ما لقيت أكلات تحتوي هذه الكلمة داخل المكونات.\"\n",
    "\n",
    "    # 3) Long question → RAG\n",
    "    if \"rag_answer\" in globals():\n",
    "        return rag_answer(msg, model=MODEL_NAME)\n",
    "\n",
    "    return \"rag_answer غير معرّفة. شغّلي خلية RAG (FAISS + Ollama) أولاً.\"\n",
    "\n",
    "\n",
    "# ===================== QUICK TESTS =====================\n",
    "print(fixed_router(\"لحم\"))\n",
    "print(\"----\")\n",
    "print(fixed_router(\"1\"))\n",
    "print(\"----\")\n",
    "print(fixed_router(\"الجريش\"))\n",
    "\n",
    "\n",
    "# ===================== SAVE SAME CODE TO FILE =====================\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"router.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "import re\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\\\u064B-\\\\u065F\\\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    text = re.sub(r\"\\\\s+\",\" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def find_recipes_by_name(user_text: str, recipes):\n",
    "    q = normalize_ar(user_text)\n",
    "    hits = []\n",
    "    for r in recipes:\n",
    "        name_n = normalize_ar(r.get(\"name\",\"\"))\n",
    "        if name_n and (q == name_n or q in name_n or name_n in q):\n",
    "            hits.append(r)\n",
    "    hits.sort(key=lambda x: len(normalize_ar(x.get(\"name\",\"\"))), reverse=True)\n",
    "    return hits\n",
    "\n",
    "def find_recipes_by_term(term: str, recipes):\n",
    "    t = normalize_ar(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    pattern = re.compile(rf\"(^|\\\\s){re.escape(t)}(\\\\s|$)\")\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = normalize_ar(\" \".join(r.get(\"ingredients\", [])))\n",
    "        if pattern.search(ing_text):\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "def format_recipe(r: dict) -> str:\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\\\n\"\n",
    "    if r.get(\"type\"): out += f\"نوع الأكلة: {r['type']}\\\\n\"\n",
    "    if r.get(\"region\"): out += f\"المنطقة: {r['region']}\\\\n\"\n",
    "    if r.get(\"cook_method\"): out += f\"طريقة الطهي: {r['cook_method']}\\\\n\"\n",
    "    if r.get(\"description\"): out += \"\\\\nالوصف:\\\\n\" + r[\"description\"].strip()\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\\\n\\\\nالمكونات:\\\\n\"\n",
    "        for ing in r[\"ingredients\"]:\n",
    "            out += f\"- {ing}\\\\n\"\n",
    "    if r.get(\"prep\"): out += \"\\\\nطريقة التحضير:\\\\n\" + r[\"prep\"].strip()\n",
    "    return out.strip()\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0a17bdb-95f7-46bd-80aa-33b1dd11c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الحمضوت — (DF-034)\n",
      "نوع الأكلة: أكلة تقليدية\n",
      "المنطقة: محافظة ظفار\n",
      "طريقة الطهي: طبخ بطيء\n",
      "\n",
      "الوصف:\n",
      "الحمضوت أكلة ظفارية شهيرة تُحضَّر من القطميم (السمن البلدي الأبيض) مع جريش القمح وقليل من الملح. تُطبخ عادةً لتحويل القطميم إلى سمن عربي، وتُقدَّم في صحن ويمكن أكلها بالخبز أو بمفردها، ويكون طعمها مالحًا.\n",
      "\n",
      "المكونات:\n",
      "- القطميم (السمن البلدي الأبيض)\n",
      "- جريش القمح\n",
      "- الملح\n",
      "\n",
      "طريقة التحضير:\n",
      "يُطبخ القطميم مع جريش القمح وقليل من الملح حتى يتكوّن خليط متماسك. تُقدَّم مباشرة بعد النضج. تكفي كم شخص: حسب الكمية\n",
      "----\n",
      "المقديد — (DF-024)\n",
      "نوع الأكلة: لحم محفوظ / طبق تقليدي\n",
      "المنطقة: محافظة ظفار\n",
      "طريقة الطهي: تمليح وتجفيف\n",
      "\n",
      "الوصف:\n",
      "المقديد من الأكلات التقليدية المعروفة في محافظة ظفار، ويُعد من طرق حفظ اللحم القديمة التي اعتمد عليها الأهالي قديمًا. يُحضَّر المقديد من شرائح اللحم التي تُملّح وتُجفَّف في الهواء الطلق لفترة حتى تنشف تمامًا، مما يسمح بحفظها لفترات طويلة دون الحاجة إلى التبريد. لا يرتبط المقديد بمناسبة معيّنة، بل يُحضَّر غالبًا للاستعمال لاحقًا عند الحاجة، ويُؤكل كوجبة رئيسية بعد طبخه أو تسخينه، وغالبًا ما يُقدَّم مع الخبز أو الأرز. ويعكس هذا الطبق جانبًا من الحكمة الغذائية في المطبخ الظفاري التقليدي وقدرته على التكيّف مع ظروف الحياة القديمة.\n",
      "\n",
      "المكونات:\n",
      "- 6 كيلو لحم\n",
      "- 3 أكواب ملح\n",
      "\n",
      "طريقة التحضير:\n",
      "يُزال الدهن من اللحم. يُقطّع اللحم شرائح بالطول دون فصلها تمامًا. تُعمل شقوق خفيفة وتُملأ بالملح وتُفرك جيدًا. تُعلّق الشرائح في الهواء الطلق وتُغطّى بشاش نظيف. تُترك حتى تنشف تمامًا ثم تُحفظ للاستعمال. تكفي كم شخص: حسب الاستخدام (للتخزين الطويل)\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\clean_display.py\n"
     ]
    }
   ],
   "source": [
    "# This cell cleans recipe text for display:\n",
    "# It removes any part that starts with \"الكلمات المفتاحية:\" from description/prep,\n",
    "# then formats the recipe without showing keywords.\n",
    "# At the end, it tests the output in Jupyter and saves the same logic into backend/clean_display.py\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Cut any text starting from the \"keywords\" label (even if it appears mid-line)\n",
    "def strip_keywords_anywhere(text: str) -> str:\n",
    "    # Return empty string if input is empty\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Split the text at the first occurrence of the keywords label and keep the part before it\n",
    "    text = re.split(r\"(?:الكلمات\\s+المفتاحية|كلمات\\s+مفتاحية)\\s*:\\s*\", text, maxsplit=1)[0]\n",
    "    # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Clean a recipe dict to ensure keywords are not leaked inside description/prep\n",
    "def clean_recipe_for_display(r: dict) -> dict:\n",
    "    # Make a shallow copy so we do not modify the original recipe object\n",
    "    rr = dict(r)\n",
    "    # Clean description field\n",
    "    rr[\"description\"] = strip_keywords_anywhere(rr.get(\"description\", \"\"))\n",
    "    # Clean preparation steps field\n",
    "    rr[\"prep\"] = strip_keywords_anywhere(rr.get(\"prep\", \"\"))\n",
    "    # Return cleaned recipe dict\n",
    "    return rr\n",
    "\n",
    "# Format a recipe for display (keywords never shown)\n",
    "def format_recipe(r: dict) -> str:\n",
    "    # Clean the recipe before formatting\n",
    "    r = clean_recipe_for_display(r)\n",
    "\n",
    "    # Build header line\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\n\"\n",
    "\n",
    "    # Add meta fields if they exist\n",
    "    if r.get(\"type\"):\n",
    "        out += f\"نوع الأكلة: {r['type']}\\n\"\n",
    "    if r.get(\"region\"):\n",
    "        out += f\"المنطقة: {r['region']}\\n\"\n",
    "    if r.get(\"cook_method\"):\n",
    "        out += f\"طريقة الطهي: {r['cook_method']}\\n\"\n",
    "\n",
    "    # Add description if available\n",
    "    if r.get(\"description\"):\n",
    "        out += \"\\nالوصف:\\n\" + r[\"description\"].strip()\n",
    "\n",
    "    # Add ingredients list if available\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\n\\nالمكونات:\\n\"\n",
    "        for ing in r[\"ingredients\"]:\n",
    "            out += f\"- {ing}\\n\"\n",
    "\n",
    "    # Add preparation steps if available\n",
    "    if r.get(\"prep\"):\n",
    "        out += \"\\nطريقة التحضير:\\n\" + r[\"prep\"].strip()\n",
    "\n",
    "    # Return final formatted text\n",
    "    return out.strip()\n",
    "\n",
    "# ===================== QUICK TESTS (JUPYTER OUTPUT) =====================\n",
    "# These prints should NOT show the \"الكلمات المفتاحية\" label\n",
    "print(format_recipe(next(r for r in recipes if r[\"id\"] == \"DF-034\")))  # الحمضوت\n",
    "print(\"----\")\n",
    "print(format_recipe(next(r for r in recipes if r[\"id\"] == \"DF-024\")))  # المقديد\n",
    "\n",
    "# ===================== SAVE SAME CODE TO FILE =====================\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"clean_display.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "import re\n",
    "\n",
    "def strip_keywords_anywhere(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.split(r\"(?:الكلمات\\\\s+المفتاحية|كلمات\\\\s+مفتاحية)\\\\s*:\\\\s*\", text, maxsplit=1)[0]\n",
    "    return text.strip()\n",
    "\n",
    "def clean_recipe_for_display(r: dict) -> dict:\n",
    "    rr = dict(r)\n",
    "    rr[\"description\"] = strip_keywords_anywhere(rr.get(\"description\", \"\"))\n",
    "    rr[\"prep\"] = strip_keywords_anywhere(rr.get(\"prep\", \"\"))\n",
    "    return rr\n",
    "\n",
    "def format_recipe(r: dict) -> str:\n",
    "    r = clean_recipe_for_display(r)\n",
    "\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\\\n\"\n",
    "    if r.get(\"type\"): out += f\"نوع الأكلة: {r['type']}\\\\n\"\n",
    "    if r.get(\"region\"): out += f\"المنطقة: {r['region']}\\\\n\"\n",
    "    if r.get(\"cook_method\"): out += f\"طريقة الطهي: {r['cook_method']}\\\\n\"\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        out += \"\\\\nالوصف:\\\\n\" + r[\"description\"].strip()\n",
    "\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\\\n\\\\nالمكونات:\\\\n\"\n",
    "        for ing in r[\"ingredients\"]:\n",
    "            out += f\"- {ing}\\\\n\"\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        out += \"\\\\nطريقة التحضير:\\\\n\" + r[\"prep\"].strip()\n",
    "\n",
    "    return out.strip()\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aba69163-c55b-4bbd-9d32-c2cf1c3eb0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\router_safe.py\n"
     ]
    }
   ],
   "source": [
    "# This cell makes the router safer:\n",
    "# It calls RAG in a protected way (try/except) so the UI will not crash if RAG fails.\n",
    "# It defines a safe_rag wrapper, a fixed_router_safe that uses it, and a chat_fn for the UI.\n",
    "# At the end, it saves the same logic into backend/router_safe.py\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the Ollama model name you have installed\n",
    "MODEL_NAME = \"llama3\"  # You can change to \"llama3.1\" if you have it\n",
    "\n",
    "def safe_rag(msg: str):\n",
    "    # If rag_answer is not defined, return a clear message instead of crashing\n",
    "    if \"rag_answer\" not in globals():\n",
    "        return \"RAG غير جاهز. شغّلي خلية (FAISS + rag_answer) أولاً.\"\n",
    "    try:\n",
    "        # Call RAG normally\n",
    "        return rag_answer(msg, model=MODEL_NAME)\n",
    "    except Exception as e:\n",
    "        # Catch errors so the interface stays working\n",
    "        return f\"صار خطأ في RAG: {type(e).__name__} — {e}\"\n",
    "\n",
    "def fixed_router_safe(msg: str):\n",
    "    # Main router that uses safe_rag instead of calling RAG directly\n",
    "    msg = (msg or \"\").strip()\n",
    "    if not msg:\n",
    "        return \"اكتبي مكوّن (لحم/سمك/تمر) أو اسم أكلة أو سؤال.\"\n",
    "\n",
    "    # 0) If user picks a number from the last options list\n",
    "    if msg.isdigit() and STATE.get(\"options\"):\n",
    "        idx = int(msg) - 1\n",
    "        opts = STATE[\"options\"]\n",
    "        if 0 <= idx < len(opts):\n",
    "            picked = opts[idx]\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(picked)\n",
    "        return \"اختاري رقم صحيح من القائمة.\"\n",
    "\n",
    "    # 1) Short message: keyword search in ingredients only\n",
    "    if is_short_term(msg):\n",
    "        term_hits = find_recipes_by_term(msg)\n",
    "        if term_hits:\n",
    "            STATE[\"options\"] = term_hits\n",
    "            out = f\"أكلات تحتوي داخل المكونات فقط ({msg}):\\n\\n\"\n",
    "            for i, r in enumerate(term_hits[:12], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "\n",
    "        # If nothing found, fallback to safe RAG instead of returning 'not found'\n",
    "        return safe_rag(msg)\n",
    "\n",
    "    # 2) Search by recipe name\n",
    "    name_hits = find_recipes_by_name(msg)\n",
    "    if name_hits:\n",
    "        if len(name_hits) == 1:\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(name_hits[0])\n",
    "\n",
    "        STATE[\"options\"] = name_hits\n",
    "        out = \"لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\\n\\n\"\n",
    "        for i, r in enumerate(name_hits, 1):\n",
    "            out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "        out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "        return out\n",
    "\n",
    "    # 3) Any other case: use safe RAG fallback\n",
    "    return safe_rag(msg)\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    # Gradio ChatInterface expects a function that returns one text string\n",
    "    return fixed_router_safe(message)\n",
    "\n",
    "# ===================== SAVE SAME CODE TO FILE =====================\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"router_safe.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "MODEL_NAME = \"llama3\"\n",
    "\n",
    "def safe_rag(msg: str):\n",
    "    if \"rag_answer\" not in globals():\n",
    "        return \"RAG غير جاهز. شغّلي خلية (FAISS + rag_answer) أولاً.\"\n",
    "    try:\n",
    "        return rag_answer(msg, model=MODEL_NAME)\n",
    "    except Exception as e:\n",
    "        return f\"صار خطأ في RAG: {type(e).__name__} — {e}\"\n",
    "\n",
    "def fixed_router_safe(msg: str):\n",
    "    msg = (msg or \"\").strip()\n",
    "    if not msg:\n",
    "        return \"اكتبي مكوّن (لحم/سمك/تمر) أو اسم أكلة أو سؤال.\"\n",
    "\n",
    "    if msg.isdigit() and STATE.get(\"options\"):\n",
    "        idx = int(msg) - 1\n",
    "        opts = STATE[\"options\"]\n",
    "        if 0 <= idx < len(opts):\n",
    "            picked = opts[idx]\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(picked)\n",
    "        return \"اختاري رقم صحيح من القائمة.\"\n",
    "\n",
    "    if is_short_term(msg):\n",
    "        term_hits = find_recipes_by_term(msg)\n",
    "        if term_hits:\n",
    "            STATE[\"options\"] = term_hits\n",
    "            out = f\"أكلات تحتوي داخل المكونات فقط ({msg}):\\\\n\\\\n\"\n",
    "            for i, r in enumerate(term_hits[:12], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\\\n\"\n",
    "            out += \"\\\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "        return safe_rag(msg)\n",
    "\n",
    "    name_hits = find_recipes_by_name(msg)\n",
    "    if name_hits:\n",
    "        if len(name_hits) == 1:\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(name_hits[0])\n",
    "\n",
    "        STATE[\"options\"] = name_hits\n",
    "        out = \"لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\\\\n\\\\n\"\n",
    "        for i, r in enumerate(name_hits, 1):\n",
    "            out += f\"{i}) {r['name']} — ({r['id']})\\\\n\"\n",
    "        out += \"\\\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "        return out\n",
    "\n",
    "    return safe_rag(msg)\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    return fixed_router_safe(message)\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e363800e-0847-49e4-a289-bc2663ce0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\ollama_client.py\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\keyword_router.py\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\vectorstore_faiss.py\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\rag_pipeline.py\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\router.py\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\clean_display.py\n",
      "OK  C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\router_safe.py\n",
      "\n",
      "Saved files: 7 / 7\n",
      "Save completion %: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "#نتاكد انه كل شئ محفوظ\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "files = [\n",
    "    PROJECT_ROOT / \"backend\" / \"ollama_client.py\",\n",
    "    PROJECT_ROOT / \"backend\" / \"keyword_router.py\",\n",
    "    PROJECT_ROOT / \"backend\" / \"vectorstore_faiss.py\",\n",
    "    PROJECT_ROOT / \"backend\" / \"rag_pipeline.py\",\n",
    "    PROJECT_ROOT / \"backend\" / \"router.py\",\n",
    "    PROJECT_ROOT / \"backend\" / \"clean_display.py\",\n",
    "    PROJECT_ROOT / \"backend\" / \"router_safe.py\",\n",
    "]\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "ok = 0\n",
    "for f in files:\n",
    "    exists = f.exists()\n",
    "    print((\"OK \" if exists else \"MISSING \"), f)\n",
    "    ok += 1 if exists else 0\n",
    "\n",
    "print(\"\\nSaved files:\", ok, \"/\", len(files))\n",
    "print(\"Save completion %:\", round((ok/len(files))*100, 1), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be738156-e456-4c36-9ba8-0cdb6bd13cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is ready.\n",
      "OK\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\check_ollama.py\n"
     ]
    }
   ],
   "source": [
    "# This cell checks Ollama readiness:\n",
    "# It verifies Ollama is installed, the service is running, the model exists,\n",
    "# then runs a quick test prompt. At the end, it saves the same code into backend/check_ollama.py\n",
    "\n",
    "import subprocess, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def check_ollama(model=\"llama3\"):\n",
    "    # Check if the ollama executable exists in PATH\n",
    "    if shutil.which(\"ollama\") is None:\n",
    "        return \"Ollama is not found in PATH. Install Ollama, then open a new terminal.\"\n",
    "\n",
    "    # Check if the Ollama service responds to 'ollama list'\n",
    "    r = subprocess.run([\"ollama\", \"list\"], text=True, capture_output=True, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    if r.returncode != 0:\n",
    "        return \"Ollama is installed but not running. Run: ollama serve\\n\" + (r.stderr or \"\").strip()[:400]\n",
    "\n",
    "    # Check if the requested model exists in the local list\n",
    "    out = (r.stdout or \"\").strip()\n",
    "    if model not in out:\n",
    "        return f\"Ollama is running but model ({model}) is not available. Run: ollama pull {model}\\n\\nAvailable models:\\n{out}\"\n",
    "\n",
    "    # Quick run test to confirm generation works\n",
    "    t = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=\"Say only: OK\",\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    if t.returncode == 0 and (t.stdout or \"\").strip():\n",
    "        return \"Ollama is ready.\\n\" + t.stdout.strip()[:200]\n",
    "\n",
    "    return \"Ollama is running but the test prompt returned an empty response.\\n\" + (t.stderr or \"\").strip()[:300]\n",
    "\n",
    "# Run now (Jupyter output)\n",
    "print(check_ollama(\"llama3\"))\n",
    "\n",
    "# Save the same code to a file\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"check_ollama.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "import subprocess, shutil\n",
    "\n",
    "def check_ollama(model=\"llama3\"):\n",
    "    if shutil.which(\"ollama\") is None:\n",
    "        return \"Ollama is not found in PATH. Install Ollama, then open a new terminal.\"\n",
    "\n",
    "    r = subprocess.run([\"ollama\", \"list\"], text=True, capture_output=True, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    if r.returncode != 0:\n",
    "        return \"Ollama is installed but not running. Run: ollama serve\\\\n\" + (r.stderr or \"\").strip()[:400]\n",
    "\n",
    "    out = (r.stdout or \"\").strip()\n",
    "    if model not in out:\n",
    "        return f\"Ollama is running but model ({model}) is not available. Run: ollama pull {model}\\\\n\\\\nAvailable models:\\\\n{out}\"\n",
    "\n",
    "    t = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=\"Say only: OK\",\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    if t.returncode == 0 and (t.stdout or \"\").strip():\n",
    "        return \"Ollama is ready.\\\\n\" + t.stdout.strip()[:200]\n",
    "\n",
    "    return \"Ollama is running but the test prompt returned an empty response.\\\\n\" + (t.stderr or \"\").strip()[:300]\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "989ade64-4818-4125-82d3-d645a81b8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أكلات تحتوي داخل المكونات فقط:\n",
      "\n",
      "1) شوربة الجريش — (DF-001)\n",
      "2) سوب اللحم والطماطم — (DF-002)\n",
      "3) كمباه مقشّد — (DF-008)\n",
      "4) القصابية — (DF-011)\n",
      "5) الجريــش باللحم — (DF-012)\n",
      "6) رز مقزّح — (DF-013)\n",
      "7) لحم مفور — (DF-022)\n",
      "8) المضبي — (DF-023)\n",
      "9) المقديد — (DF-024)\n",
      "10) المعجين — (DF-025)\n",
      "11) قبولي — (DF-040)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "----\n",
      "ما لقيت داخل المكونات، لكن لقيتها في البحث العام:\n",
      "\n",
      "1) شوربة الجريش — (DF-001)\n",
      "2) خبز الثخين — (DF-003)\n",
      "3) العطراية — (DF-005)\n",
      "4) خبز لَحوح — (DF-007)\n",
      "5) عيش بالنارجيل — (DF-010)\n",
      "6) القصابية — (DF-011)\n",
      "7) الجريــش باللحم — (DF-012)\n",
      "8) رز مقزّح — (DF-013)\n",
      "9) لبنية الكزيب — (DF-015)\n",
      "10) الربيس — (DF-017)\n",
      "11) الصيادية — (DF-018)\n",
      "12) القشاط — (DF-019)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "----\n",
      "ما لقيت داخل المكونات، لكن لقيتها في البحث العام:\n",
      "\n",
      "1) المقديد — (DF-024)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\engine.py\n"
     ]
    }
   ],
   "source": [
    "# This cell defines the final engine:\n",
    "# It combines strict ingredient matching, name search, general search, and RAG (FAISS + Ollama).\n",
    "# It also runs quick tests in Jupyter and saves the same code into backend/engine.py.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ===================== NORMALIZE =====================\n",
    "def normalize_ar(text: str) -> str:\n",
    "    # Normalize Arabic text for consistent matching\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")  # Normalize Alef forms\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")  # Normalize Ya and Ta Marbuta\n",
    "    text = re.sub(r\"\\s+\",\" \", text).strip().lower()  # Clean spaces and lowercase\n",
    "    return text\n",
    "\n",
    "\n",
    "# ===================== TOKENIZE (ingredients only) =====================\n",
    "def tokenize_ar(text: str):\n",
    "    # Tokenize Arabic text into a set of words, with optional \"ال\" removal\n",
    "    t = normalize_ar(text)\n",
    "    t = re.sub(r\"[^0-9\\u0621-\\u064A]+\", \" \", t)  # Keep Arabic letters and numbers only\n",
    "    words = [w for w in t.split() if w]  # Split into words\n",
    "    tokens = set(words)  # Use set for fast lookup\n",
    "    for w in words:\n",
    "        if w.startswith(\"ال\") and len(w) > 2:\n",
    "            tokens.add(w[2:])  # Add the word without \"ال\"\n",
    "    return tokens\n",
    "\n",
    "def canonical_term(term: str):\n",
    "    # Normalize a single search term and remove \"ال\" if it exists\n",
    "    t = normalize_ar(term)\n",
    "    t = re.sub(r\"[^0-9\\u0621-\\u064A]+\", \" \", t).strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    if t.startswith(\"ال\") and len(t) > 2:\n",
    "        return t[2:]\n",
    "    return t\n",
    "\n",
    "\n",
    "# ===================== STRICT ingredient search (100%) =====================\n",
    "def find_recipes_by_ingredient_strict(term: str):\n",
    "    # Strictly match the term as a full token inside ingredients only\n",
    "    t = canonical_term(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = \" \".join(r.get(\"ingredients\", []))  # Ingredients only\n",
    "        tokens = tokenize_ar(ing_text)  # Token set of ingredients\n",
    "        if t in tokens:  # Exact token match\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== general search (name/type/region/desc/ingredients) =====================\n",
    "def find_recipes_general(term: str):\n",
    "    # General substring search across multiple fields (for words like \"تقليدي\")\n",
    "    t = normalize_ar(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        hay = \" \".join([\n",
    "            r.get(\"name\",\"\"),\n",
    "            r.get(\"type\",\"\") or \"\",\n",
    "            r.get(\"region\",\"\") or \"\",\n",
    "            r.get(\"cook_method\",\"\") or \"\",\n",
    "            r.get(\"description\",\"\") or \"\",\n",
    "            \" \".join(r.get(\"ingredients\", []))\n",
    "        ])\n",
    "        if t in normalize_ar(hay):\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== name search (user input inside name) =====================\n",
    "def find_recipes_by_name(user_text: str):\n",
    "    # Find recipes where user text appears in the recipe name\n",
    "    q = normalize_ar(user_text)\n",
    "    hits = []\n",
    "    for r in recipes:\n",
    "        name_n = normalize_ar(r.get(\"name\",\"\"))\n",
    "        if not name_n:\n",
    "            continue\n",
    "        if q == name_n or (q and q in name_n):\n",
    "            hits.append(r)\n",
    "    hits.sort(key=lambda x: len(normalize_ar(x.get(\"name\",\"\"))), reverse=True)\n",
    "    return hits\n",
    "\n",
    "\n",
    "# ===================== format =====================\n",
    "def format_recipe(r: dict) -> str:\n",
    "    # Format a recipe into a response text (without emojis)\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\n\"\n",
    "    if r.get(\"type\"): out += f\"نوع الأكلة: {r['type']}\\n\"\n",
    "    if r.get(\"region\"): out += f\"المنطقة: {r['region']}\\n\"\n",
    "    if r.get(\"cook_method\"): out += f\"طريقة الطهي: {r['cook_method']}\\n\"\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        out += \"\\nالوصف:\\n\" + r[\"description\"].strip()\n",
    "\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\n\\nالمكونات:\\n\" + \"\\n\".join([f\"- {x}\" for x in r[\"ingredients\"]])\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        out += \"\\n\\nطريقة التحضير:\\n\" + r[\"prep\"].strip()\n",
    "\n",
    "    return out.strip()\n",
    "\n",
    "def is_short_term(msg: str) -> bool:\n",
    "    # One or two words is treated as a keyword search\n",
    "    return len((msg or \"\").split()) <= 2\n",
    "\n",
    "\n",
    "# ===================== RAG (FAISS + Ollama) =====================\n",
    "MODEL_NAME = \"llama3\"  # Change only if you use a different local model\n",
    "\n",
    "DOMAIN_DISCLAIMER = (\n",
    "    \"تنبيه: هذا الشات يجيب فقط من وثائق (وصفات ظفار) داخل المشروع. \"\n",
    "    \"إذا المعلومة غير موجودة في الوثائق، سيتم إرجاع: غير موجود في البيانات.\"\n",
    ")\n",
    "\n",
    "def retrieve_top_chunks(query: str, top_k: int = 4):\n",
    "    # Embed the query and search the FAISS index\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "\n",
    "    chunks = []\n",
    "    for idx in I[0]:\n",
    "        chunks.append(texts[idx])\n",
    "    return chunks\n",
    "\n",
    "def ollama_generate(prompt: str, model=MODEL_NAME):\n",
    "    # Call Ollama and return the model output\n",
    "    r = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    return (r.stdout or \"\").strip()\n",
    "\n",
    "def rag_answer(user_q: str, model=MODEL_NAME):\n",
    "    # Retrieve context and ask the model to answer using context only\n",
    "    chunks = retrieve_top_chunks(user_q, top_k=4)\n",
    "    context = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{DOMAIN_DISCLAIMER}\n",
    "\n",
    "قواعد مهمة:\n",
    "- استخدم (السياق) فقط.\n",
    "- إذا لم تجد الإجابة في السياق، اكتب بالضبط: غير موجود في البيانات\n",
    "- لا تخمّن ولا تضف معلومات خارج السياق.\n",
    "\n",
    "السياق:\n",
    "{context}\n",
    "\n",
    "سؤال المستخدم:\n",
    "{user_q}\n",
    "\n",
    "الإجابة:\n",
    "\"\"\"\n",
    "    out = ollama_generate(prompt, model=model)\n",
    "    return out if out else \"غير موجود في البيانات\"\n",
    "\n",
    "\n",
    "# ===================== STATE + ROUTER =====================\n",
    "STATE = {\"options\": None}\n",
    "\n",
    "def fixed_router(msg: str):\n",
    "    # Main router that combines strict ingredients, general search, name search, and RAG\n",
    "    msg = (msg or \"\").strip()\n",
    "    if not msg:\n",
    "        return \"اكتبي مكوّن (لحم/سمك/تمر) أو كلمة عامة (تقليدي/تراث) أو اسم أكلة.\"\n",
    "\n",
    "    # (0) User selects a number from the last options list\n",
    "    if msg.isdigit() and STATE[\"options\"]:\n",
    "        idx = int(msg) - 1\n",
    "        opts = STATE[\"options\"]\n",
    "        if 0 <= idx < len(opts):\n",
    "            picked = opts[idx]\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(picked)\n",
    "        return \"اختاري رقم صحيح من القائمة.\"\n",
    "\n",
    "    # (1) Short keyword: strict ingredient search first\n",
    "    if is_short_term(msg):\n",
    "        strict_hits = find_recipes_by_ingredient_strict(msg)\n",
    "        if strict_hits:\n",
    "            STATE[\"options\"] = strict_hits[:12]\n",
    "            out = \"أكلات تحتوي داخل المكونات فقط:\\n\\n\"\n",
    "            for i, r in enumerate(STATE[\"options\"], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "\n",
    "        # If not found in ingredients, use general search\n",
    "        general_hits = find_recipes_general(msg)\n",
    "        if general_hits:\n",
    "            STATE[\"options\"] = general_hits[:12]\n",
    "            out = \"ما لقيت داخل المكونات، لكن لقيتها في البحث العام:\\n\\n\"\n",
    "            for i, r in enumerate(STATE[\"options\"], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "\n",
    "        return \"ما لقيت نتائج.\"\n",
    "\n",
    "    # (2) Longer text: try name search first\n",
    "    name_hits = find_recipes_by_name(msg)\n",
    "    if name_hits:\n",
    "        if len(name_hits) == 1:\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(name_hits[0])\n",
    "\n",
    "        STATE[\"options\"] = name_hits[:12]\n",
    "        out = \"لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\\n\\n\"\n",
    "        for i, r in enumerate(STATE[\"options\"], 1):\n",
    "            out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "        out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "        return out\n",
    "\n",
    "    # (3) Fallback: use RAG when no clear name match\n",
    "    return rag_answer(msg, model=MODEL_NAME)\n",
    "\n",
    "\n",
    "# ===================== quick tests (Jupyter output) =====================\n",
    "print(fixed_router(\"لحم\"))\n",
    "print(\"----\")\n",
    "print(fixed_router(\"تقليدي\"))\n",
    "print(\"----\")\n",
    "print(fixed_router(\"المقديد\"))\n",
    "\n",
    "\n",
    "# ===================== SAVE SAME CODE TO FILE =====================\n",
    "PROJECT_ROOT = Path(r\"C:\\\\Users\\\\USER PC\\\\OneDrive\\\\Desktop\\\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"engine.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = r'''import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    text = re.sub(r\"\\s+\",\" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def tokenize_ar(text: str):\n",
    "    t = normalize_ar(text)\n",
    "    t = re.sub(r\"[^0-9\\u0621-\\u064A]+\", \" \", t)\n",
    "    words = [w for w in t.split() if w]\n",
    "    tokens = set(words)\n",
    "    for w in words:\n",
    "        if w.startswith(\"ال\") and len(w) > 2:\n",
    "            tokens.add(w[2:])\n",
    "    return tokens\n",
    "\n",
    "def canonical_term(term: str):\n",
    "    t = normalize_ar(term)\n",
    "    t = re.sub(r\"[^0-9\\u0621-\\u064A]+\", \" \", t).strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    if t.startswith(\"ال\") and len(t) > 2:\n",
    "        return t[2:]\n",
    "    return t\n",
    "\n",
    "def find_recipes_by_ingredient_strict(term: str, recipes):\n",
    "    t = canonical_term(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = \" \".join(r.get(\"ingredients\", []))\n",
    "        tokens = tokenize_ar(ing_text)\n",
    "        if t in tokens:\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "def find_recipes_general(term: str, recipes):\n",
    "    t = normalize_ar(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        hay = \" \".join([\n",
    "            r.get(\"name\",\"\"),\n",
    "            r.get(\"type\",\"\") or \"\",\n",
    "            r.get(\"region\",\"\") or \"\",\n",
    "            r.get(\"cook_method\",\"\") or \"\",\n",
    "            r.get(\"description\",\"\") or \"\",\n",
    "            \" \".join(r.get(\"ingredients\", []))\n",
    "        ])\n",
    "        if t in normalize_ar(hay):\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "def find_recipes_by_name(user_text: str, recipes):\n",
    "    q = normalize_ar(user_text)\n",
    "    hits = []\n",
    "    for r in recipes:\n",
    "        name_n = normalize_ar(r.get(\"name\",\"\"))\n",
    "        if not name_n:\n",
    "            continue\n",
    "        if q == name_n or (q and q in name_n):\n",
    "            hits.append(r)\n",
    "    hits.sort(key=lambda x: len(normalize_ar(x.get(\"name\",\"\"))), reverse=True)\n",
    "    return hits\n",
    "\n",
    "def format_recipe(r: dict) -> str:\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\n\"\n",
    "    if r.get(\"type\"): out += f\"نوع الأكلة: {r['type']}\\n\"\n",
    "    if r.get(\"region\"): out += f\"المنطقة: {r['region']}\\n\"\n",
    "    if r.get(\"cook_method\"): out += f\"طريقة الطهي: {r['cook_method']}\\n\"\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        out += \"\\nالوصف:\\n\" + r[\"description\"].strip()\n",
    "\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\n\\nالمكونات:\\n\" + \"\\n\".join([f\"- {x}\" for x in r[\"ingredients\"]])\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        out += \"\\n\\nطريقة التحضير:\\n\" + r[\"prep\"].strip()\n",
    "\n",
    "    return out.strip()\n",
    "\n",
    "def is_short_term(msg: str) -> bool:\n",
    "    return len((msg or \"\").split()) <= 2\n",
    "\n",
    "MODEL_NAME = \"llama3\"\n",
    "\n",
    "DOMAIN_DISCLAIMER = (\n",
    "    \"تنبيه: هذا الشات يجيب فقط من وثائق (وصفات ظفار) داخل المشروع. \"\n",
    "    \"إذا المعلومة غير موجودة في الوثائق، سيتم إرجاع: غير موجود في البيانات.\"\n",
    ")\n",
    "\n",
    "def retrieve_top_chunks(query: str, top_k: int, embedder, index, texts):\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    return [texts[idx] for idx in I[0]]\n",
    "\n",
    "def ollama_generate(prompt: str, model=MODEL_NAME):\n",
    "    r = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    return (r.stdout or \"\").strip()\n",
    "\n",
    "def rag_answer(user_q: str, embedder, index, texts, top_k: int = 4, model=MODEL_NAME):\n",
    "    chunks = retrieve_top_chunks(user_q, top_k=top_k, embedder=embedder, index=index, texts=texts)\n",
    "    context = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{DOMAIN_DISCLAIMER}\n",
    "\n",
    "قواعد مهمة:\n",
    "- استخدم (السياق) فقط.\n",
    "- إذا لم تجد الإجابة في السياق، اكتب بالضبط: غير موجود في البيانات\n",
    "- لا تخمّن ولا تضف معلومات خارج السياق.\n",
    "\n",
    "السياق:\n",
    "{context}\n",
    "\n",
    "سؤال المستخدم:\n",
    "{user_q}\n",
    "\n",
    "الإجابة:\n",
    "\"\"\"\n",
    "    out = ollama_generate(prompt, model=model)\n",
    "    return out if out else \"غير موجود في البيانات\"\n",
    "\n",
    "STATE = {\"options\": None}\n",
    "\n",
    "def fixed_router(msg: str, recipes, embedder=None, index=None, texts=None):\n",
    "    msg = (msg or \"\").strip()\n",
    "    if not msg:\n",
    "        return \"اكتبي مكوّن (لحم/سمك/تمر) أو كلمة عامة (تقليدي/تراث) أو اسم أكلة.\"\n",
    "\n",
    "    if msg.isdigit() and STATE[\"options\"]:\n",
    "        idx = int(msg) - 1\n",
    "        opts = STATE[\"options\"]\n",
    "        if 0 <= idx < len(opts):\n",
    "            picked = opts[idx]\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(picked)\n",
    "        return \"اختاري رقم صحيح من القائمة.\"\n",
    "\n",
    "    if is_short_term(msg):\n",
    "        strict_hits = find_recipes_by_ingredient_strict(msg, recipes)\n",
    "        if strict_hits:\n",
    "            STATE[\"options\"] = strict_hits[:12]\n",
    "            out = \"أكلات تحتوي داخل المكونات فقط:\\n\\n\"\n",
    "            for i, r in enumerate(STATE[\"options\"], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "\n",
    "        general_hits = find_recipes_general(msg, recipes)\n",
    "        if general_hits:\n",
    "            STATE[\"options\"] = general_hits[:12]\n",
    "            out = \"ما لقيت داخل المكونات، لكن لقيتها في البحث العام:\\n\\n\"\n",
    "            for i, r in enumerate(STATE[\"options\"], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "\n",
    "        return \"ما لقيت نتائج.\"\n",
    "\n",
    "    name_hits = find_recipes_by_name(msg, recipes)\n",
    "    if name_hits:\n",
    "        if len(name_hits) == 1:\n",
    "            STATE[\"options\"] = None\n",
    "            return format_recipe(name_hits[0])\n",
    "\n",
    "        STATE[\"options\"] = name_hits[:12]\n",
    "        out = \"لقيت أكثر من أكلة مطابقة للااسم، اختاري رقم:\\n\\n\"\n",
    "        for i, r in enumerate(STATE[\"options\"], 1):\n",
    "            out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "        out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "        return out\n",
    "\n",
    "    if embedder is not None and index is not None and texts is not None:\n",
    "        return rag_answer(msg, embedder=embedder, index=index, texts=texts, top_k=4, model=MODEL_NAME)\n",
    "\n",
    "    return \"RAG غير جاهز. وفري embedder/index/texts أو شغلي خلية FAISS.\"\n",
    "'''\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76beb929-8106-4709-bd22-3bcec3eeeb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes sanitized: leaked text removed from description/prep/ingredients.\n",
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\sanitize_recipes.py\n"
     ]
    }
   ],
   "source": [
    "# This cell cleans and sanitizes the parsed recipes list.\n",
    "# It removes any leaked text that accidentally got attached to description/prep/ingredients\n",
    "# (مثل: \"الكلمات المفتاحية\", \"تكفي كم شخص\", أو بداية وصفة ثانية داخل نفس النص).\n",
    "# It runs the cleaning now in Jupyter, then saves the same logic into backend/sanitize_recipes.py.\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Clean a single text field (description/prep/ingredient line) from leaked next-recipe text\n",
    "def _clean_text_field(text: str) -> str:\n",
    "    # Return empty string if the input is missing\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to string and remove extra spaces\n",
    "    t = str(text).strip()\n",
    "\n",
    "    # Cut the text if these markers appear inside the field\n",
    "    cut_markers = [\n",
    "        r\"\\bتكفي\\s*كم\\s*شخص\\b\",\n",
    "        r\"\\bالكلمات\\s*المفتاحية\\b\",\n",
    "    ]\n",
    "    for m in cut_markers:\n",
    "        t = re.split(m, t, maxsplit=1)[0].strip()\n",
    "\n",
    "    # Cut if a new recipe title line appears inside the text (example: \"... — (DF-026)\")\n",
    "    t = re.split(\n",
    "        r\"\\n\\s*[\\u0600-\\u06FF0-9 ()\\-\\–—_]+?\\s*—\\s*\\(DF-\\d+\\)\\s*\\n\",\n",
    "        t,\n",
    "        maxsplit=1\n",
    "    )[0].strip()\n",
    "\n",
    "    # Cut if a new recipe ID appears inside the text (a sign of a new record)\n",
    "    t = re.split(r\"\\n\\s*ID\\s*:\\s*DF-\\d+\\s*\\n\", t, maxsplit=1)[0].strip()\n",
    "    t = re.split(r\"\\n\\s*DF-\\d+\\s*\\n\", t, maxsplit=1)[0].strip()\n",
    "\n",
    "    # Reduce very long blank-line sequences\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    return t\n",
    "\n",
    "# Clean the ingredients list and remove leading bullet symbols\n",
    "def _clean_ingredients_list(ings):\n",
    "    # Return empty list if ingredients are missing\n",
    "    if not ings:\n",
    "        return []\n",
    "    cleaned = []\n",
    "    for x in ings:\n",
    "        # Clean each ingredient line\n",
    "        s = _clean_text_field(x)\n",
    "        # Remove bullet prefixes like \"-\" or \"•\"\n",
    "        s = re.sub(r\"^\\s*[-•]+\\s*\", \"\", s).strip()\n",
    "        if s:\n",
    "            cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "# Sanitize all recipes in-place and remove unwanted keys\n",
    "def sanitize_recipes(recipes_list):\n",
    "    \"\"\"\n",
    "    Removes leaked text (keywords/serves/next recipe) if it got attached to a recipe.\n",
    "    Run once after recipes parsing.\n",
    "    \"\"\"\n",
    "    for r in recipes_list:\n",
    "        # Clean basic text fields\n",
    "        r[\"name\"] = (r.get(\"name\") or \"\").strip()\n",
    "        r[\"description\"] = _clean_text_field(r.get(\"description\", \"\"))\n",
    "        r[\"prep\"] = _clean_text_field(r.get(\"prep\", \"\"))\n",
    "\n",
    "        # Clean ingredients list\n",
    "        r[\"ingredients\"] = _clean_ingredients_list(r.get(\"ingredients\", []))\n",
    "\n",
    "        # Remove keys that should not exist or should not be shown\n",
    "        for k in [\"keywords\", \"key_words\", \"tags\", \"serves\", \"portion\", \"تكفي\", \"كلمات_مفتاحية\"]:\n",
    "            if k in r:\n",
    "                r.pop(k, None)\n",
    "\n",
    "    return recipes_list\n",
    "\n",
    "# Run now (Jupyter output)\n",
    "recipes = sanitize_recipes(recipes)\n",
    "print(\"Recipes sanitized: leaked text removed from description/prep/ingredients.\")\n",
    "\n",
    "# Save the same code to a file\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"sanitize_recipes.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code_to_save = \"\"\"\\\n",
    "import re\n",
    "\n",
    "def _clean_text_field(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = str(text).strip()\n",
    "\n",
    "    cut_markers = [\n",
    "        r\"\\\\bتكفي\\\\s*كم\\\\s*شخص\\\\b\",\n",
    "        r\"\\\\bالكلمات\\\\s*المفتاحية\\\\b\",\n",
    "    ]\n",
    "    for m in cut_markers:\n",
    "        t = re.split(m, t, maxsplit=1)[0].strip()\n",
    "\n",
    "    t = re.split(\n",
    "        r\"\\\\n\\\\s*[\\\\u0600-\\\\u06FF0-9 ()\\\\-\\\\–—_]+?\\\\s*—\\\\s*\\\\(DF-\\\\d+\\\\)\\\\s*\\\\n\",\n",
    "        t,\n",
    "        maxsplit=1\n",
    "    )[0].strip()\n",
    "\n",
    "    t = re.split(r\"\\\\n\\\\s*ID\\\\s*:\\\\s*DF-\\\\d+\\\\s*\\\\n\", t, maxsplit=1)[0].strip()\n",
    "    t = re.split(r\"\\\\n\\\\s*DF-\\\\d+\\\\s*\\\\n\", t, maxsplit=1)[0].strip()\n",
    "\n",
    "    t = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", t).strip()\n",
    "    return t\n",
    "\n",
    "def _clean_ingredients_list(ings):\n",
    "    if not ings:\n",
    "        return []\n",
    "    cleaned = []\n",
    "    for x in ings:\n",
    "        s = _clean_text_field(x)\n",
    "        s = re.sub(r\"^\\\\s*[-•]+\\\\s*\", \"\", s).strip()\n",
    "        if s:\n",
    "            cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "def sanitize_recipes(recipes_list):\n",
    "    for r in recipes_list:\n",
    "        r[\"name\"] = (r.get(\"name\") or \"\").strip()\n",
    "        r[\"description\"] = _clean_text_field(r.get(\"description\", \"\"))\n",
    "        r[\"prep\"] = _clean_text_field(r.get(\"prep\", \"\"))\n",
    "\n",
    "        r[\"ingredients\"] = _clean_ingredients_list(r.get(\"ingredients\", []))\n",
    "\n",
    "        for k in [\"keywords\", \"key_words\", \"tags\", \"serves\", \"portion\", \"تكفي\", \"كلمات_مفتاحية\"]:\n",
    "            if k in r:\n",
    "                r.pop(k, None)\n",
    "\n",
    "    return recipes_list\n",
    "\"\"\"\n",
    "\n",
    "TARGET_FILE.write_text(code_to_save, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "901c6400-20eb-415a-9f78-3d2b97d348ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\backend\\app_engine.py\n"
     ]
    }
   ],
   "source": [
    "# This cell creates ONE final backend module that merges all improvements into one file.\n",
    "# It writes backend/app_engine.py which includes:\n",
    "# - recipe sanitization (remove leaked text)\n",
    "# - clean display (remove keywords if leaked)\n",
    "# - strict ingredient matching (100% token match)\n",
    "# - name search + general search\n",
    "# - safe RAG wrapper (FAISS + Ollama)\n",
    "# - a stateful Engine class for chat routing\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "TARGET_FILE = PROJECT_ROOT / \"backend\" / \"app_engine.py\"\n",
    "TARGET_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "code = r'''\n",
    "import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# ===================== NORMALIZATION =====================\n",
    "def normalize_ar(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")  # Normalize Alef\n",
    "    text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")  # Normalize Ya / Ta Marbuta\n",
    "    text = re.sub(r\"\\s+\",\" \", text).strip().lower()  # Clean spaces and lowercase\n",
    "    return text\n",
    "\n",
    "# ===================== SANITIZE RECIPES (remove leaked next-recipe text) =====================\n",
    "def _clean_text_field(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    t = str(text).strip()\n",
    "\n",
    "    # Cut if markers appear inside description/prep\n",
    "    cut_markers = [\n",
    "        r\"\\bتكفي\\s*كم\\s*شخص\\b\",\n",
    "        r\"\\bالكلمات\\s*المفتاحية\\b\",\n",
    "    ]\n",
    "    for m in cut_markers:\n",
    "        t = re.split(m, t, maxsplit=1)[0].strip()\n",
    "\n",
    "    # Cut if a new recipe title line appears inside the text (example: \"... — (DF-026)\")\n",
    "    t = re.split(r\"\\n\\s*[\\u0600-\\u06FF0-9 ()\\-\\–—_]+?\\s*—\\s*\\(DF-\\d+\\)\\s*\\n\", t, maxsplit=1)[0].strip()\n",
    "\n",
    "    # Cut if a new recipe ID appears inside the text\n",
    "    t = re.split(r\"\\n\\s*ID\\s*:\\s*DF-\\d+\\s*\\n\", t, maxsplit=1)[0].strip()\n",
    "    t = re.split(r\"\\n\\s*DF-\\d+\\s*\\n\", t, maxsplit=1)[0].strip()\n",
    "\n",
    "    # Clean extra blank lines\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    return t\n",
    "\n",
    "def _clean_ingredients_list(ings):\n",
    "    if not ings:\n",
    "        return []\n",
    "    cleaned = []\n",
    "    for x in ings:\n",
    "        s = _clean_text_field(x)\n",
    "        s = re.sub(r\"^\\s*[-•]+\\s*\", \"\", s).strip()  # Remove bullet prefixes\n",
    "        if s:\n",
    "            cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "def sanitize_recipes(recipes_list):\n",
    "    # Clean recipes in-place and remove unwanted keys\n",
    "    for r in recipes_list:\n",
    "        r[\"name\"] = (r.get(\"name\") or \"\").strip()\n",
    "        r[\"description\"] = _clean_text_field(r.get(\"description\", \"\"))\n",
    "        r[\"prep\"] = _clean_text_field(r.get(\"prep\", \"\"))\n",
    "        r[\"ingredients\"] = _clean_ingredients_list(r.get(\"ingredients\", []))\n",
    "\n",
    "        for k in [\"keywords\", \"key_words\", \"tags\", \"serves\", \"portion\", \"تكفي\", \"كلمات_مفتاحية\"]:\n",
    "            if k in r:\n",
    "                r.pop(k, None)\n",
    "    return recipes_list\n",
    "\n",
    "# ===================== CLEAN DISPLAY (strip keywords if leaked) =====================\n",
    "def strip_keywords_anywhere(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.split(r\"(?:الكلمات\\s+المفتاحية|كلمات\\s+مفتاحية)\\s*:\\s*\", text, maxsplit=1)[0]\n",
    "    return text.strip()\n",
    "\n",
    "def clean_recipe_for_display(r: dict) -> dict:\n",
    "    rr = dict(r)\n",
    "    rr[\"description\"] = strip_keywords_anywhere(rr.get(\"description\", \"\"))\n",
    "    rr[\"prep\"] = strip_keywords_anywhere(rr.get(\"prep\", \"\"))\n",
    "    return rr\n",
    "\n",
    "# ===================== TOKENIZE (ingredients only) =====================\n",
    "def tokenize_ar(text: str):\n",
    "    t = normalize_ar(text)\n",
    "    t = re.sub(r\"[^0-9\\u0621-\\u064A]+\", \" \", t)  # Keep Arabic letters and numbers only\n",
    "    words = [w for w in t.split() if w]\n",
    "    tokens = set(words)\n",
    "    for w in words:\n",
    "        if w.startswith(\"ال\") and len(w) > 2:\n",
    "            tokens.add(w[2:])  # Add without \"ال\"\n",
    "    return tokens\n",
    "\n",
    "def canonical_term(term: str):\n",
    "    t = normalize_ar(term)\n",
    "    t = re.sub(r\"[^0-9\\u0621-\\u064A]+\", \" \", t).strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    return t[2:] if t.startswith(\"ال\") and len(t) > 2 else t\n",
    "\n",
    "# ===================== SEARCH =====================\n",
    "def find_recipes_by_ingredient_strict(term: str, recipes):\n",
    "    # Strict token match in ingredients only\n",
    "    t = canonical_term(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        ing_text = \" \".join(r.get(\"ingredients\", []))\n",
    "        tokens = tokenize_ar(ing_text)\n",
    "        if t in tokens:\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "def find_recipes_general(term: str, recipes):\n",
    "    # General substring search across multiple fields\n",
    "    t = normalize_ar(term)\n",
    "    if not t:\n",
    "        return []\n",
    "    out = []\n",
    "    for r in recipes:\n",
    "        hay = \" \".join([\n",
    "            r.get(\"name\",\"\"),\n",
    "            r.get(\"type\",\"\") or \"\",\n",
    "            r.get(\"region\",\"\") or \"\",\n",
    "            r.get(\"cook_method\",\"\") or \"\",\n",
    "            r.get(\"description\",\"\") or \"\",\n",
    "            \" \".join(r.get(\"ingredients\", []))\n",
    "        ])\n",
    "        if t in normalize_ar(hay):\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "def find_recipes_by_name(user_text: str, recipes):\n",
    "    # Find if user text exists inside recipe name\n",
    "    q = normalize_ar(user_text)\n",
    "    hits = []\n",
    "    for r in recipes:\n",
    "        name_n = normalize_ar(r.get(\"name\",\"\"))\n",
    "        if not name_n:\n",
    "            continue\n",
    "        if q == name_n or (q and q in name_n):\n",
    "            hits.append(r)\n",
    "    hits.sort(key=lambda x: len(normalize_ar(x.get(\"name\",\"\"))), reverse=True)\n",
    "    return hits\n",
    "\n",
    "# ===================== FORMAT =====================\n",
    "def format_recipe(r: dict) -> str:\n",
    "    # Format recipe for display (no keywords leaked)\n",
    "    r = clean_recipe_for_display(r)\n",
    "\n",
    "    out = f\"{r.get('name','')} — ({r.get('id','')})\\n\"\n",
    "    if r.get(\"type\"): out += f\"نوع الأكلة: {r['type']}\\n\"\n",
    "    if r.get(\"region\"): out += f\"المنطقة: {r['region']}\\n\"\n",
    "    if r.get(\"cook_method\"): out += f\"طريقة الطهي: {r['cook_method']}\\n\"\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        out += \"\\nالوصف:\\n\" + r[\"description\"].strip()\n",
    "\n",
    "    if r.get(\"ingredients\"):\n",
    "        out += \"\\n\\nالمكونات:\\n\" + \"\\n\".join([f\"- {x}\" for x in r[\"ingredients\"]])\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        out += \"\\n\\nطريقة التحضير:\\n\" + r[\"prep\"].strip()\n",
    "\n",
    "    return out.strip()\n",
    "\n",
    "def is_short_term(msg: str) -> bool:\n",
    "    return len((msg or \"\").split()) <= 2\n",
    "\n",
    "# ===================== RAG (FAISS + Ollama) =====================\n",
    "DOMAIN_DISCLAIMER = (\n",
    "    \"تنبيه: هذا الشات يجيب فقط من وثائق (وصفات ظفار) داخل المشروع. \"\n",
    "    \"إذا المعلومة غير موجودة في الوثائق، سيتم إرجاع: غير موجود في البيانات.\"\n",
    ")\n",
    "\n",
    "def retrieve_top_chunks(query: str, embedder, index, texts, top_k: int = 4):\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "    _, I = index.search(q_emb, top_k)\n",
    "    return [texts[idx] for idx in I[0]]\n",
    "\n",
    "def ollama_generate(prompt: str, model: str = \"llama3\") -> str:\n",
    "    r = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    return (r.stdout or \"\").strip()\n",
    "\n",
    "def rag_answer(user_q: str, embedder, index, texts, model: str = \"llama3\", top_k: int = 4) -> str:\n",
    "    chunks = retrieve_top_chunks(user_q, embedder, index, texts, top_k=top_k)\n",
    "    context = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "    prompt = f\"\"\"\n",
    "{DOMAIN_DISCLAIMER}\n",
    "\n",
    "قواعد مهمة:\n",
    "- استخدم (السياق) فقط.\n",
    "- إذا لم تجد الإجابة في السياق، اكتب بالضبط: غير موجود في البيانات\n",
    "- لا تخمّن ولا تضف معلومات خارج السياق.\n",
    "\n",
    "السياق:\n",
    "{context}\n",
    "\n",
    "سؤال المستخدم:\n",
    "{user_q}\n",
    "\n",
    "الإجابة:\n",
    "\"\"\"\n",
    "    out = ollama_generate(prompt, model=model)\n",
    "    return out if out else \"غير موجود في البيانات\"\n",
    "\n",
    "# ===================== STATEFUL ENGINE =====================\n",
    "class Engine:\n",
    "    # This class stores state and routes messages\n",
    "    def __init__(self, recipes, embedder=None, index=None, texts=None, model_name=\"llama3\"):\n",
    "        self.recipes = recipes\n",
    "        self.embedder = embedder\n",
    "        self.index = index\n",
    "        self.texts = texts\n",
    "        self.model_name = model_name\n",
    "        self.state = {\"options\": None}\n",
    "\n",
    "    def safe_rag(self, msg: str) -> str:\n",
    "        # If RAG is not available, return a safe message\n",
    "        if self.embedder is None or self.index is None or self.texts is None:\n",
    "            return \"RAG غير جاهز. شغّلي خلية FAISS + texts أولاً.\"\n",
    "        try:\n",
    "            return rag_answer(msg, self.embedder, self.index, self.texts, model=self.model_name, top_k=4)\n",
    "        except Exception as e:\n",
    "            return f\"صار خطأ في RAG: {type(e).__name__} — {e}\"\n",
    "\n",
    "    def chat(self, msg: str) -> str:\n",
    "        # Main router logic with state\n",
    "        msg = (msg or \"\").strip()\n",
    "        if not msg:\n",
    "            return \"اكتبي مكوّن (لحم/سمك/تمر) أو كلمة عامة (تقليدي/تراث) أو اسم أكلة.\"\n",
    "\n",
    "        # If user selects a number from options\n",
    "        if msg.isdigit() and self.state.get(\"options\"):\n",
    "            idx = int(msg) - 1\n",
    "            opts = self.state[\"options\"]\n",
    "            if 0 <= idx < len(opts):\n",
    "                picked = opts[idx]\n",
    "                self.state[\"options\"] = None\n",
    "                return format_recipe(picked)\n",
    "            return \"اختاري رقم صحيح من القائمة.\"\n",
    "\n",
    "        # Short term: strict ingredients first, then general, then safe RAG\n",
    "        if is_short_term(msg):\n",
    "            strict_hits = find_recipes_by_ingredient_strict(msg, self.recipes)\n",
    "            if strict_hits:\n",
    "                self.state[\"options\"] = strict_hits[:12]\n",
    "                out = \"أكلات تحتوي داخل المكونات فقط:\\n\\n\"\n",
    "                for i, r in enumerate(self.state[\"options\"], 1):\n",
    "                    out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "                out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "                return out\n",
    "\n",
    "            general_hits = find_recipes_general(msg, self.recipes)\n",
    "            if general_hits:\n",
    "                self.state[\"options\"] = general_hits[:12]\n",
    "                out = \"نتائج من البحث العام:\\n\\n\"\n",
    "                for i, r in enumerate(self.state[\"options\"], 1):\n",
    "                    out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "                out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "                return out\n",
    "\n",
    "            return self.safe_rag(msg)\n",
    "\n",
    "        # Longer input: try name search, else safe RAG\n",
    "        name_hits = find_recipes_by_name(msg, self.recipes)\n",
    "        if name_hits:\n",
    "            if len(name_hits) == 1:\n",
    "                self.state[\"options\"] = None\n",
    "                return format_recipe(name_hits[0])\n",
    "\n",
    "            self.state[\"options\"] = name_hits[:12]\n",
    "            out = \"لقيت أكثر من أكلة مطابقة للاسم، اختاري رقم:\\n\\n\"\n",
    "            for i, r in enumerate(self.state[\"options\"], 1):\n",
    "                out += f\"{i}) {r['name']} — ({r['id']})\\n\"\n",
    "            out += \"\\nاكتبي رقم الأكلة لعرض التفاصيل.\"\n",
    "            return out\n",
    "\n",
    "        return self.safe_rag(msg)\n",
    "'''\n",
    "\n",
    "TARGET_FILE.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Saved:\", TARGET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed3aea09-1006-4750-a5a5-15a2f14bd111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abb2b43322f41628f99d89e6532adbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed recipes: 44\n",
      "Test 1: أكلات تحتوي داخل المكونات فقط:\n",
      "\n",
      "1) مرق السمك بالنارجيل — (DF-014)\n",
      "2) عطراية (الحبار) — (DF-016)\n",
      "3) الربيس — (DF-017)\n",
      "4) الصيادية — (DF-018)\n",
      "5) المضبي — (DF-023)\n",
      "6) المالح — (DF-036)\n",
      "\n",
      "اكتبي رقم الأكلة لعرض التفاصيل.\n",
      "----\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell runs the Gradio UI and uses backend/app_engine.py as the chatbot engine.\n",
    "# It loads the DOCX domain, parses recipes, sanitizes them, builds FAISS,\n",
    "# then tests the engine and launches Gradio.\n",
    "\n",
    "!pip -q install gradio python-docx sentence-transformers faiss-cpu\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "DOCX_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"dhofar flavor.docx\"\n",
    "\n",
    "# Import the merged engine module\n",
    "import sys\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from backend.app_engine import Engine, sanitize_recipes\n",
    "\n",
    "# Read DOCX as raw text\n",
    "def read_docx_text(docx_path: str) -> str:\n",
    "    doc = Document(docx_path)\n",
    "    parts = []\n",
    "    for p in doc.paragraphs:\n",
    "        t = (p.text or \"\").strip()\n",
    "        if t:\n",
    "            parts.append(t)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# Parse recipes from raw text (same logic you used previously)\n",
    "def parse_recipes_from_docx_text(raw_text: str):\n",
    "    lines = [l.strip() for l in raw_text.splitlines() if l.strip()]\n",
    "    id_pat = re.compile(r\"^ID\\s*:\\s*(DF-[A-Za-z0-9\\-]+)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "    recipes = []\n",
    "    cur = None\n",
    "    section = None\n",
    "    last_text_line = None\n",
    "\n",
    "    def new_recipe():\n",
    "        return {\n",
    "            \"id\": None,\n",
    "            \"name\": None,\n",
    "            \"type\": None,\n",
    "            \"region\": None,\n",
    "            \"cook_method\": None,\n",
    "            \"description\": \"\",\n",
    "            \"ingredients\": [],\n",
    "            \"prep\": \"\",\n",
    "            \"keywords\": []\n",
    "        }\n",
    "\n",
    "    def normalize_ar_local(text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)\n",
    "        text = text.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "        text = text.replace(\"ى\",\"ي\").replace(\"ة\",\"ه\")\n",
    "        text = re.sub(r\"\\s+\",\" \", text).strip().lower()\n",
    "        return text\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur\n",
    "        if not cur:\n",
    "            return\n",
    "        cur[\"description\"] = (cur[\"description\"] or \"\").strip()\n",
    "        cur[\"prep\"] = (cur[\"prep\"] or \"\").strip()\n",
    "        cur[\"keywords\"] = list(set(\n",
    "            normalize_ar_local(w)\n",
    "            for w in ((cur[\"name\"] or \"\").split() + (cur[\"type\"] or \"\").split())\n",
    "            if w\n",
    "        ))\n",
    "        if cur.get(\"id\"):\n",
    "            recipes.append(cur)\n",
    "        cur = None\n",
    "\n",
    "    for line in lines:\n",
    "        m = id_pat.match(line)\n",
    "        if m:\n",
    "            flush()\n",
    "            cur = new_recipe()\n",
    "            cur[\"id\"] = m.group(1).strip()\n",
    "            if last_text_line and \":\" not in last_text_line:\n",
    "                cur[\"name\"] = last_text_line.strip()\n",
    "            else:\n",
    "                cur[\"name\"] = None\n",
    "            section = None\n",
    "            continue\n",
    "\n",
    "        if not line.startswith((\"نوع الأكلة\", \"المنطقة\", \"طريقة الطهي\", \"الوصف\", \"المكونات\", \"طريقة التحضير\")):\n",
    "            if len(line) <= 60:\n",
    "                last_text_line = line\n",
    "\n",
    "        if not cur:\n",
    "            continue\n",
    "\n",
    "        if \"نوع الأكلة\" in line:\n",
    "            cur[\"type\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "        if \"المنطقة\" in line:\n",
    "            cur[\"region\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "        if \"طريقة الطهي\" in line:\n",
    "            cur[\"cook_method\"] = line.split(\":\", 1)[-1].strip()\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"الوصف\"):\n",
    "            section = \"description\"\n",
    "            continue\n",
    "        if line.startswith(\"المكونات\"):\n",
    "            section = \"ingredients\"\n",
    "            continue\n",
    "        if line.startswith(\"طريقة التحضير\"):\n",
    "            section = \"prep\"\n",
    "            continue\n",
    "\n",
    "        if section == \"description\":\n",
    "            cur[\"description\"] += line + \" \"\n",
    "        elif section == \"ingredients\":\n",
    "            if \":\" not in line and not line.startswith((\"ملاحظات\", \"تكفي\")):\n",
    "                cur[\"ingredients\"].append(line.strip())\n",
    "        elif section == \"prep\":\n",
    "            cur[\"prep\"] += line + \" \"\n",
    "\n",
    "    flush()\n",
    "    return recipes\n",
    "\n",
    "# Load and build everything\n",
    "if not DOCX_PATH.exists():\n",
    "    raise FileNotFoundError(f\"DOCX not found: {DOCX_PATH}\")\n",
    "\n",
    "raw_text = read_docx_text(str(DOCX_PATH))\n",
    "recipes = parse_recipes_from_docx_text(raw_text)\n",
    "recipes = sanitize_recipes(recipes)\n",
    "\n",
    "# Build texts for FAISS\n",
    "texts = []\n",
    "for r in recipes:\n",
    "    doc = (\n",
    "        f\"اسم: {r.get('name','')}\\n\"\n",
    "        f\"ID: {r.get('id','')}\\n\"\n",
    "        f\"نوع: {r.get('type','')}\\n\"\n",
    "        f\"المنطقة: {r.get('region','')}\\n\"\n",
    "        f\"طريقة الطهي: {r.get('cook_method','')}\\n\"\n",
    "        f\"الوصف: {r.get('description','')}\\n\"\n",
    "        f\"المكونات: {', '.join(r.get('ingredients', [])[:25])}\\n\"\n",
    "        f\"طريقة التحضير: {r.get('prep','')}\"\n",
    "    )\n",
    "    texts.append(doc)\n",
    "\n",
    "# Embeddings + FAISS\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "emb = embedder.encode(texts, show_progress_bar=True)\n",
    "emb = np.array(emb, dtype=np.float32)\n",
    "\n",
    "index = faiss.IndexFlatL2(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "# Create the engine\n",
    "engine = Engine(recipes=recipes, embedder=embedder, index=index, texts=texts, model_name=\"llama3\")\n",
    "\n",
    "# Quick tests (Jupyter output)\n",
    "print(\"Parsed recipes:\", len(recipes))\n",
    "print(\"Test 1:\", engine.chat(\"سمك\"))\n",
    "print(\"----\")\n",
    "\n",
    "# Gradio UI\n",
    "def chat_fn(message, history):\n",
    "    return engine.chat(message)\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"Dhofar Flavor Chat\",\n",
    "    description=(\n",
    "        \"اكتبي مكوّن (لحم/سمك/تمر) لاقتراح أكلات.\\n\"\n",
    "        \"اكتبي اسم أكلة لعرض تفاصيلها.\\n\"\n",
    "        \"اكتبي سؤال طويل ليتم البحث باستخدام RAG.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76e36ddf-0ef0-456a-8390-a60a471245d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved recipes.pkl at: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\data\\processed\\recipes.pkl\n",
      "Recipes count: 44\n",
      "FAISS NOT saved because variable 'vectordb' not found.\n",
      "If your web engine uses LangChain FAISS.load_local(), you must build vectordb then run this cell.\n",
      "Tip: build vectordb using LangChain FAISS.from_texts(...) or FAISS.from_documents(...).\n"
     ]
    }
   ],
   "source": [
    "# This cell creates required folders and saves:\n",
    "# 1) data/processed/recipes.pkl from the \"recipes\" variable\n",
    "# 2) vectorstore/faiss_index/index.faiss + index.pkl using LangChain FAISS if \"vectordb\" exists\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Change only this if your project folder is different\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Target paths\n",
    "RECIPES_PKL = PROJECT_ROOT / \"data\" / \"processed\" / \"recipes.pkl\"\n",
    "FAISS_DIR   = PROJECT_ROOT / \"vectorstore\" / \"faiss_index\"\n",
    "\n",
    "# Create folders\n",
    "RECIPES_PKL.parent.mkdir(parents=True, exist_ok=True)\n",
    "FAISS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Save recipes.pkl\n",
    "# -----------------------------\n",
    "if \"recipes\" not in globals() or not isinstance(recipes, list) or len(recipes) == 0:\n",
    "    print(\"recipes.pkl NOT saved because variable 'recipes' is missing or empty.\")\n",
    "    print(\"Run your parsing cell first so 'recipes' becomes a list of recipe dicts.\")\n",
    "else:\n",
    "    with open(RECIPES_PKL, \"wb\") as f:\n",
    "        pickle.dump(recipes, f)\n",
    "    print(\"Saved recipes.pkl at:\", str(RECIPES_PKL))\n",
    "    print(\"Recipes count:\", len(recipes))\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Save FAISS index for the WEB engine (LangChain format)\n",
    "# This must create: index.faiss + index.pkl\n",
    "# -----------------------------\n",
    "if \"vectordb\" in globals() and vectordb is not None:\n",
    "    try:\n",
    "        # LangChain FAISS object has save_local()\n",
    "        vectordb.save_local(str(FAISS_DIR))\n",
    "        print(\"Saved LangChain FAISS at:\", str(FAISS_DIR))\n",
    "        print(\"Expected files:\",\n",
    "              (FAISS_DIR / \"index.faiss\").exists(),\n",
    "              (FAISS_DIR / \"index.pkl\").exists())\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save LangChain FAISS:\", type(e).__name__, \"-\", str(e)[:300])\n",
    "        print(\"Make sure 'vectordb' is a LangChain FAISS object (not faiss.Index).\")\n",
    "else:\n",
    "    print(\"FAISS NOT saved because variable 'vectordb' not found.\")\n",
    "    print(\"If your web engine uses LangChain FAISS.load_local(), you must build vectordb then run this cell.\")\n",
    "    print(\"Tip: build vectordb using LangChain FAISS.from_texts(...) or FAISS.from_documents(...).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85175821-e3d9-44df-817c-410244e48a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built successfully.\n",
      "FAISS_PATH: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\vectorstore\\faiss_index\n",
      "EMBED_OLLAMA_MODEL: nomic-embed-text\n",
      "Documents: 44\n",
      "index.faiss exists: True\n",
      "index.pkl exists: True\n"
     ]
    }
   ],
   "source": [
    "# This cell builds a LangChain FAISS index from data/processed/recipes.pkl\n",
    "# and saves it to: vectorstore/faiss_index/index.faiss + index.pkl\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Project root (change only if your folder is different)\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\")\n",
    "\n",
    "# Paths (same structure used by rag_engine.py)\n",
    "FAISS_PATH_ENV = os.getenv(\"FAISS_PATH\", str(PROJECT_ROOT / \"vectorstore\" / \"faiss_index\"))\n",
    "FAISS_PATH = FAISS_PATH_ENV if os.path.isabs(FAISS_PATH_ENV) else str(PROJECT_ROOT / FAISS_PATH_ENV)\n",
    "\n",
    "EMBED_OLLAMA_MODEL = os.getenv(\"EMBED_OLLAMA_MODEL\", \"nomic-embed-text\")\n",
    "\n",
    "RECIPES_PKL = PROJECT_ROOT / \"data\" / \"processed\" / \"recipes.pkl\"\n",
    "if not RECIPES_PKL.exists():\n",
    "    raise FileNotFoundError(f\"recipes.pkl not found at: {RECIPES_PKL}\")\n",
    "\n",
    "with open(RECIPES_PKL, \"rb\") as f:\n",
    "    recipes = pickle.load(f)\n",
    "\n",
    "# Remove anything after \"الكلمات المفتاحية:\" if it leaked into description/prep\n",
    "def strip_keywords_anywhere(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "    t = re.split(r\"(?:الكلمات\\s+المفتاحية|كلمات\\s+مفتاحية)\\s*:\\s*\", t, maxsplit=1)[0]\n",
    "    return t.strip()\n",
    "\n",
    "def clean_recipe_for_rag(r: dict) -> dict:\n",
    "    rr = dict(r)\n",
    "    rr[\"description\"] = strip_keywords_anywhere(rr.get(\"description\", \"\"))\n",
    "    rr[\"prep\"] = strip_keywords_anywhere(rr.get(\"prep\", \"\"))\n",
    "    return rr\n",
    "\n",
    "# Arabic normalization (for aliases only)\n",
    "def normalize_ar(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "# Aliases improve retrieval quality for known spelling variants (example: القماحة)\n",
    "def build_name_aliases(dish_title: str):\n",
    "    title = (dish_title or \"\").strip()\n",
    "    if not title:\n",
    "        return []\n",
    "    title_n = normalize_ar(title)\n",
    "\n",
    "    if any(x in title_n for x in [\"القماحه\", \"قماحه\", \"القماحة\", \"قماحة\", \"قمحه\"]):\n",
    "        return [\"القماحة\", \"قماحة\", \"قماحه\", \"قمحه\"]\n",
    "\n",
    "    return []\n",
    "\n",
    "# Build Documents for FAISS\n",
    "docs = []\n",
    "for r in recipes:\n",
    "    r = clean_recipe_for_rag(r)\n",
    "\n",
    "    dish_id = (r.get(\"id\") or \"\").strip()\n",
    "    dish_title = (r.get(\"name\") or \"\").strip()\n",
    "    region = (r.get(\"region\") or \"\").strip()\n",
    "\n",
    "    text_parts = []\n",
    "\n",
    "    if dish_title:\n",
    "        text_parts.append(f\"اسم الطبق: {dish_title}\")\n",
    "\n",
    "        aliases = build_name_aliases(dish_title)\n",
    "        if aliases:\n",
    "            text_parts.append(\"اسماء بديلة:\\n\" + \"\\n\".join([f\"- {a}\" for a in aliases]))\n",
    "\n",
    "    if dish_id:\n",
    "        text_parts.append(f\"ID: {dish_id}\")\n",
    "\n",
    "    if region:\n",
    "        text_parts.append(f\"المنطقة: {region}\")\n",
    "\n",
    "    if r.get(\"description\"):\n",
    "        text_parts.append(\"الوصف:\\n\" + str(r[\"description\"]).strip())\n",
    "\n",
    "    ings = r.get(\"ingredients\") or []\n",
    "    if ings:\n",
    "        text_parts.append(\"المكونات:\\n\" + \"\\n\".join([f\"- {x}\" for x in ings]))\n",
    "\n",
    "    if r.get(\"prep\"):\n",
    "        text_parts.append(\"طريقة التحضير:\\n\" + str(r[\"prep\"]).strip())\n",
    "\n",
    "    content = \"\\n\\n\".join(text_parts).strip()\n",
    "\n",
    "    docs.append(\n",
    "        Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": \"recipes.pkl\",\n",
    "                \"dish_id\": dish_id,\n",
    "                \"dish_title\": dish_title,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Build + save LangChain FAISS\n",
    "embeddings = OllamaEmbeddings(model=EMBED_OLLAMA_MODEL)\n",
    "\n",
    "os.makedirs(FAISS_PATH, exist_ok=True)\n",
    "\n",
    "# Remove old index files if they exist\n",
    "for fn in [\"index.faiss\", \"index.pkl\"]:\n",
    "    fp = os.path.join(FAISS_PATH, fn)\n",
    "    if os.path.exists(fp):\n",
    "        os.remove(fp)\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "db.save_local(FAISS_PATH)\n",
    "\n",
    "print(\"FAISS index built successfully.\")\n",
    "print(\"FAISS_PATH:\", FAISS_PATH)\n",
    "print(\"EMBED_OLLAMA_MODEL:\", EMBED_OLLAMA_MODEL)\n",
    "print(\"Documents:\", len(docs))\n",
    "print(\"index.faiss exists:\", os.path.exists(os.path.join(FAISS_PATH, \"index.faiss\")))\n",
    "print(\"index.pkl exists:\", os.path.exists(os.path.join(FAISS_PATH, \"index.pkl\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d99e0af9-3716-465e-b445-9a1b28219888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Using cached langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-ollama) (1.2.7)\n",
      "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
      "  Using cached ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.10.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user pc\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.3.0)\n",
      "Using cached langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Using cached ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "\n",
      "   ---------------------------------------- 2/2 [langchain-ollama]\n",
      "\n",
      "Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba58038-fae6-4dd2-8595-1e9c80e0a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\data\\processed\\recipes.pkl\n",
      "🔢 Count: 44\n",
      "🧾 Keys sample: ['id', 'name', 'type', 'region', 'cook_method', 'description', 'ingredients', 'prep']\n",
      "\n",
      "--- First 10 (name / id) ---\n",
      "01) شوربة الجريش | DF-001\n",
      "02) سوب اللحم والطماطم | DF-002\n",
      "03) خبز الثخين | DF-003\n",
      "04) الفندال المقصّص | DF-004\n",
      "05) العطراية | DF-005\n",
      "06) القراص | DF-006\n",
      "07) خبز لَحوح | DF-007\n",
      "08) كمباه مقشّد | DF-008\n",
      "09) الدَّجَر مع الماش | DF-009\n",
      "10) عيش بالنارجيل | DF-010\n",
      "\n",
      "🔎 'قبولي' -> 1 result(s)\n",
      "01) قبولي | DF-040\n",
      "\n",
      "🔎 'القبولي' -> 0 result(s)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "PKL_PATH = Path(r\"C:\\Users\\USER PC\\OneDrive\\Desktop\\dhofar_flavor_chat project\\data\\processed\\recipes.pkl\")\n",
    "\n",
    "with open(PKL_PATH, \"rb\") as f:\n",
    "    recipes = pickle.load(f)\n",
    "\n",
    "print(\"✅ Loaded:\", PKL_PATH)\n",
    "print(\"🔢 Count:\", len(recipes))\n",
    "print(\"🧾 Keys sample:\", list(recipes[0].keys()))\n",
    "\n",
    "# عرض أول 10 أسماء + ID\n",
    "print(\"\\n--- First 10 (name / id) ---\")\n",
    "for i, r in enumerate(recipes[:10], 1):\n",
    "    print(f\"{i:02d}) {r.get('name','')} | {r.get('id','')}\")\n",
    "\n",
    "def search_name(q):\n",
    "    q = (q or \"\").strip()\n",
    "    hits = [r for r in recipes if q in str(r.get(\"name\",\"\"))]\n",
    "    print(f\"\\n🔎 '{q}' -> {len(hits)} result(s)\")\n",
    "    for i, r in enumerate(hits[:30], 1):\n",
    "        print(f\"{i:02d}) {r.get('name','')} | {r.get('id','')}\")\n",
    "    return hits\n",
    "\n",
    "hits1 = search_name(\"قبولي\")\n",
    "hits2 = search_name(\"القبولي\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb31f7-2858-4d04-ba16-dc16f304b673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
