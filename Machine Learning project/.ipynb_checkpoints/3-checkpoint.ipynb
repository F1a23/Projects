{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b137033d-a3d3-4009-9a89-f16d2d68dbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after cleaning: (9955, 3)\n",
      "\n",
      "Sentiment value counts (before balancing):\n",
      "Sentiment\n",
      "1    6269\n",
      "0    2494\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Before balancing:\n",
      "Positive: 6269\n",
      "Negative: 2494\n",
      "\n",
      "After balancing:\n",
      "Sentiment\n",
      "0    6269\n",
      "1    6269\n",
      "Name: count, dtype: int64\n",
      "Balanced shape: (12538, 4)\n",
      "\n",
      "Train size: 10030\n",
      "Test size: 2508\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nb_preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 106\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_test_text))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 5. BASELINE MODEL: TF-IDF + NAIVE BAYES (WITH TEXT TRICKS)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m    104\u001b[0m baseline_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m    105\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m, TfidfVectorizer(\n\u001b[1;32m--> 106\u001b[0m         preprocessor\u001b[38;5;241m=\u001b[39mnb_preprocessor,   \u001b[38;5;66;03m# âœ… Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„ÙØ¬ Ø®Ø§Øµ Ù„Ù†Ø§ÙŠÙ Ø¨Ø§ÙŠØ²\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m         ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),             \u001b[38;5;66;03m# unigrams + bigrams\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m    110\u001b[0m     )),\n\u001b[0;32m    111\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, ComplementNB(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m))  \u001b[38;5;66;03m# âœ… ComplementNB Ø¹Ø§Ø¯Ø© Ø£ÙØ¶Ù„ Ù…Ù† MultinomialNB\u001b[39;00m\n\u001b[0;32m    112\u001b[0m ])\n\u001b[0;32m    114\u001b[0m baseline_pipeline\u001b[38;5;241m.\u001b[39mfit(X_train_text, y_train)\n\u001b[0;32m    116\u001b[0m y_pred_base \u001b[38;5;241m=\u001b[39m baseline_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ============================================================\n",
    "# 0. IMPORTS\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD & CLEAN DATA\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"Restaurant reviews.csv\")\n",
    "\n",
    "# Keep only important columns\n",
    "df = df[[\"Restaurant\", \"Review\", \"Rating\"]].copy()\n",
    "\n",
    "# Drop rows with missing values in these columns\n",
    "df.dropna(subset=[\"Restaurant\", \"Review\", \"Rating\"], inplace=True)\n",
    "\n",
    "# Clean Rating column\n",
    "df[\"Rating\"] = df[\"Rating\"].astype(str).str.strip()\n",
    "\n",
    "# (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ù„Ùˆ ÙÙŠ ÙƒÙ„Ù…Ø© \"Like\" Ù†Ø­ÙˆÙ„Ù‡Ø§ Ø¥Ù„Ù‰ 4 (ØªÙ‚ÙŠÙŠÙ… Ø¥ÙŠØ¬Ø§Ø¨ÙŠ)\n",
    "df[\"Rating\"] = df[\"Rating\"].str.replace(\"Like\", \"4\", case=False, regex=False)\n",
    "\n",
    "# Convert Rating to numeric\n",
    "df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where Rating is NaN\n",
    "df.dropna(subset=[\"Rating\"], inplace=True)\n",
    "\n",
    "# Keep only ratings between 1 and 5\n",
    "df = df[(df[\"Rating\"] >= 1) & (df[\"Rating\"] <= 5)]\n",
    "\n",
    "print(\"Data shape after cleaning:\", df.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. CREATE BINARY SENTIMENT LABEL\n",
    "#    0 = Negative (1, 2)\n",
    "#    1 = Positive (4, 5)\n",
    "#    Remove neutral (3)\n",
    "# ============================================================\n",
    "df = df[df[\"Rating\"] != 3]\n",
    "\n",
    "df[\"Sentiment\"] = np.where(df[\"Rating\"] >= 4, 1, 0)\n",
    "\n",
    "print(\"\\nSentiment value counts (before balancing):\")\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. BALANCE THE DATA (POSITIVE = NEGATIVE)\n",
    "#    Over-sampling negative class\n",
    "# ============================================================\n",
    "df_pos = df[df[\"Sentiment\"] == 1]   # Positive reviews\n",
    "df_neg = df[df[\"Sentiment\"] == 0]   # Negative reviews\n",
    "\n",
    "print(\"\\nBefore balancing:\")\n",
    "print(\"Positive:\", len(df_pos))\n",
    "print(\"Negative:\", len(df_neg))\n",
    "\n",
    "# Over-sample the minority (negative) to match positive count\n",
    "df_neg_oversampled = df_neg.sample(len(df_pos), replace=True, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_balanced = pd.concat([df_pos, df_neg_oversampled], axis=0).sample(frac=1, random_state=42)\n",
    "\n",
    "print(\"\\nAfter balancing:\")\n",
    "print(df_balanced[\"Sentiment\"].value_counts())\n",
    "print(\"Balanced shape:\", df_balanced.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. TRAINâ€“TEST SPLIT (ON BALANCED DATA)\n",
    "# ============================================================\n",
    "X_text = df_balanced[\"Review\"]\n",
    "y = df_balanced[\"Sentiment\"]\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain size:\", len(X_train_text))\n",
    "print(\"Test size:\", len(X_test_text))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. BASELINE MODEL: TF-IDF + NAIVE BAYES (WITH TEXT TRICKS)\n",
    "# ============================================================\n",
    "baseline_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        preprocessor=nb_preprocessor,   # âœ… Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„ÙØ¬ Ø®Ø§Øµ Ù„Ù†Ø§ÙŠÙ Ø¨Ø§ÙŠØ²\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),             # unigrams + bigrams\n",
    "        max_features=5000\n",
    "    )),\n",
    "    (\"model\", ComplementNB(alpha=0.5))  # âœ… ComplementNB Ø¹Ø§Ø¯Ø© Ø£ÙØ¶Ù„ Ù…Ù† MultinomialNB\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train_text, y_train)\n",
    "\n",
    "y_pred_base = baseline_pipeline.predict(X_test_text)\n",
    "acc_base = accuracy_score(y_test, y_pred_base)\n",
    "\n",
    "print(\"\\n===== BASELINE MODEL (TF-IDF + Complement Naive Bayes) =====\")\n",
    "print(\"Accuracy:\", acc_base)\n",
    "print(\"\\nClassification Report (Baseline):\")\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. IMPROVED MODEL: TF-IDF + LOGISTIC REGRESSION + GRIDSEARCH\n",
    "#    L1 & L2 regularization\n",
    "# ============================================================\n",
    "logreg_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=5000\n",
    "    )),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=5000,\n",
    "        solver=\"liblinear\"   # supports L1 and L2\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],       # L1 vs L2\n",
    "    \"model__C\": [0.01, 0.1, 1, 10]        # Regularization strength\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=logreg_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_text, y_train)\n",
    "\n",
    "print(\"\\n===== GRIDSEARCH RESULTS (Logistic Regression) =====\")\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "best_logreg_model = grid.best_estimator_\n",
    "\n",
    "y_pred_grid = best_logreg_model.predict(X_test_text)\n",
    "acc_grid = accuracy_score(y_test, y_pred_grid)\n",
    "\n",
    "print(\"\\n===== BEST LOGISTIC MODEL ON TEST SET =====\")\n",
    "print(\"Accuracy:\", acc_grid)\n",
    "print(\"\\nClassification Report (Best Logistic):\")\n",
    "print(classification_report(y_test, y_pred_grid))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. FINAL MODEL COMPARISON TABLE\n",
    "# ============================================================\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"TF-IDF + Naive Bayes (Balanced Data)\",\n",
    "        \"TF-IDF + Logistic (GridSearch L1/L2, Balanced Data)\"\n",
    "    ],\n",
    "    \"Accuracy\": [acc_base, acc_grid]\n",
    "})\n",
    "\n",
    "print(\"\\n===== FINAL MODEL COMPARISON =====\")\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05a90f0-8c2d-4f82-b26c-82b604829250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== MODEL TEST ON CUSTOM REVIEWS =====\n",
      "\n",
      "=======================================\n",
      "Review: The food was excellent and the staff were incredibly friendly.\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n",
      "\n",
      "=======================================\n",
      "Review: Terrible experience, the service was slow and the food was cold.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "=======================================\n",
      "Review: It was okay, nothing special, just an average place.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "=======================================\n",
      "Review: Worst restaurant ever, I will never come back!\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "=======================================\n",
      "Review: Loved the ambience and the dishes, totally worth the money.\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n",
      "\n",
      "=======================================\n",
      "Review: The place is overrated, I expected much better quality.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d35c604-dda4-4aab-986e-8dd089c89ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 10-FOLD CROSS VALIDATION (Logistic Best Model) =====\n",
      "CV Scores: [0.96331738 0.96172249 0.95773525 0.95295056 0.94976077 0.95614035\n",
      " 0.94098884 0.94736842 0.94972067 0.94253791]\n",
      "Mean CV Accuracy: 0.9522242630446099\n",
      "Std Deviation   : 0.007179722088679674\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. STRONG TEST: 10-FOLD CROSS-VALIDATION (LOGISTIC MODEL)\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Ù†Ø³ØªØ®Ø¯Ù… ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø© (Ù…Ùˆ Ø¨Ø³ train)\n",
    "X_full = df_balanced[\"Review\"]\n",
    "y_full = df_balanced[\"Sentiment\"]\n",
    "\n",
    "# Ù†ÙØ³ Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ (best_logreg_model) Ù„Ø£Ù†Ù‡ Pipeline: TF-IDF + Logistic\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    best_logreg_model,\n",
    "    X_full,\n",
    "    y_full,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n===== 10-FOLD CROSS VALIDATION (Logistic Best Model) =====\")\n",
    "print(\"CV Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "print(\"Std Deviation   :\", cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f5ffb9-41b7-495f-bf2b-18254dd1d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== STRONG STRESS TEST ON TRICKY SENTENCES =====\n",
      "\n",
      "---------------------------------------\n",
      "Review: The food is not good at all, but the staff tried their best.\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: I thought it would be amazing, but it was not worth the hype.\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: The starters were great, but the main course was disappointing.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: Service was slow, but the food tasted really good.\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: If you like waiting for an hour for cold food, this is your place.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: Fantastic place... if you enjoy bad service and average food.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: The food was decent, but honestly I expected something better for the price.\n",
      "Naive Bayes Prediction   : Negative ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: Nice ambience, but the food didn't match the reviews I read.\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Negative ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: Absolutely loved everything about this restaurant, highly recommended!\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n",
      "\n",
      "---------------------------------------\n",
      "Review: Best restaurant in the area, amazing flavors and top-notch service!\n",
      "Naive Bayes Prediction   : Positive ğŸ‘\n",
      "Logistic (Best Model)    : Positive ğŸ‘\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. STRONG STRESS TEST ON TRICKY SENTENCES\n",
    "# ============================================================\n",
    "\n",
    "def strong_test_review(text):\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Review:\", text)\n",
    "    pred_nb = baseline_pipeline.predict([text])[0]\n",
    "    pred_log = best_logreg_model.predict([text])[0]\n",
    "\n",
    "    label_nb = \"Negative ğŸ‘\" if pred_nb == 0 else \"Positive ğŸ‘\"\n",
    "    label_log = \"Negative ğŸ‘\" if pred_log == 0 else \"Positive ğŸ‘\"\n",
    "\n",
    "    print(\"Naive Bayes Prediction   :\", label_nb)\n",
    "    print(\"Logistic (Best Model)    :\", label_log)\n",
    "\n",
    "\n",
    "hard_examples = [\n",
    "    # Ù†ÙÙŠ ØµØ±ÙŠØ­\n",
    "    \"The food is not good at all, but the staff tried their best.\",\n",
    "    \"I thought it would be amazing, but it was not worth the hype.\",\n",
    "\n",
    "    # Ø±Ø£ÙŠ Ù…Ø®ØªÙ„Ø· (Ù†Øµ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ù†Øµ Ø³Ù„Ø¨ÙŠ)\n",
    "    \"The starters were great, but the main course was disappointing.\",\n",
    "    \"Service was slow, but the food tasted really good.\",\n",
    "\n",
    "    # Ø³Ø®Ø±ÙŠØ© Ø®ÙÙŠÙØ© / ÙƒÙ„Ø§Ù… Ù…Ù„ØºÙˆÙ…\n",
    "    \"If you like waiting for an hour for cold food, this is your place.\",\n",
    "    \"Fantastic place... if you enjoy bad service and average food.\",\n",
    "\n",
    "    # Ù…Ø¯Ø­ Ù…Ø¹ ØªØ­ÙØ¸\n",
    "    \"The food was decent, but honestly I expected something better for the price.\",\n",
    "    \"Nice ambience, but the food didn't match the reviews I read.\",\n",
    "\n",
    "    # Ù…Ø¯Ø­ Ù‚ÙˆÙŠ Ø¬Ø¯Ù‹Ø§\n",
    "    \"Absolutely loved everything about this restaurant, highly recommended!\",\n",
    "    \"Best restaurant in the area, amazing flavors and top-notch service!\"\n",
    "]\n",
    "\n",
    "print(\"\\n\\n===== STRONG STRESS TEST ON TRICKY SENTENCES =====\")\n",
    "for txt in hard_examples:\n",
    "    strong_test_review(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cd760-feea-4060-baec-68566f85d74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
